{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwAQ6J6f4Gp4",
    "outputId": "7c7747d6-e220-4ad5-f141-85e39943013f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting git+https://github.com/Rottenham/PvZ-Emulator\n",
      "  Cloning https://github.com/Rottenham/PvZ-Emulator to /tmp/pip-req-build-cu49dt2c\n",
      "  Running command git clone -q https://github.com/Rottenham/PvZ-Emulator /tmp/pip-req-build-cu49dt2c\n",
      "  Running command git submodule update --init --recursive -q\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT! First, go to Runtime -> Change runtime type, select a GPU runtime\n",
    "# And then Runtime -> Run all\n",
    "!pip install git+https://github.com/Rottenham/PvZ-Emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXwXYckvViOv"
   },
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aM2hTDsUp7i"
   },
   "source": [
    "**config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O9qNPjfA4QmF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class GameStatus(Enum):\n",
    "    CONTINUE = 0\n",
    "    WIN = 1\n",
    "    LOSE = 2\n",
    "    TIMEUP = 3\n",
    "\n",
    "\n",
    "N_LANES = 5  # Height\n",
    "LANE_LENGTH = 9  # Width\n",
    "P_LANE_LENGTH = 4\n",
    "Z_LANE_LENGTH = LANE_LENGTH - P_LANE_LENGTH\n",
    "N_PLANT_TYPE = 4\n",
    "N_ZOMBIE_TYPE = 3\n",
    "SUN_MAX = 1950\n",
    "\n",
    "# action\n",
    "ACTION_SIZE = N_ZOMBIE_TYPE * N_LANES * Z_LANE_LENGTH + 1\n",
    "\n",
    "# state\n",
    "NUM_ZOMBIES = 39\n",
    "NUM_PLANTS = 20\n",
    "ZOMBIE_SIZE = 6\n",
    "PLANT_SIZE = 4\n",
    "BRAIN_BASE = NUM_ZOMBIES * ZOMBIE_SIZE + NUM_PLANTS * PLANT_SIZE + 1  # extra 1 for sun\n",
    "BRAIN_SIZE = 5\n",
    "STATE_SIZE = BRAIN_BASE + BRAIN_SIZE + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rru16G2zUsKq"
   },
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aAwHz-3D4SGV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pvzemu import (\n",
    "    World,\n",
    "    SceneType,\n",
    "    ZombieType,\n",
    "    PlantType,\n",
    "    IZObservation,\n",
    ")\n",
    "\n",
    "plant_counts = {\n",
    "    PlantType.sunflower: 9,\n",
    "    PlantType.pea_shooter: 6,\n",
    "    PlantType.squash: 3,\n",
    "    PlantType.snow_pea: 2,\n",
    "}\n",
    "\n",
    "zombie_deck = [\n",
    "    [ZombieType.zombie, 50],\n",
    "    [ZombieType.buckethead, 125],\n",
    "    [ZombieType.football, 175],\n",
    "]\n",
    "\n",
    "\n",
    "class IZenv:\n",
    "    def __init__(self, step_length=50, max_step=None):\n",
    "        self.step_length = step_length\n",
    "        self.max_step = max_step\n",
    "\n",
    "        self.ob_factory = IZObservation(NUM_ZOMBIES, NUM_PLANTS)\n",
    "        self.state = []\n",
    "        self.zombie_count, self.plant_count, self.brains = 0, 0, [0, 1, 2, 3, 4]\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.sun_spent = 0\n",
    "        self.world = World(SceneType.night)\n",
    "        self._reset_world()\n",
    "\n",
    "    def reset(self):\n",
    "        self.zombie_count, self.plant_count, self.brains = 0, 0, [0, 1, 2, 3, 4]\n",
    "        self.step_count = 0\n",
    "        self.sun_spent = 0\n",
    "        self._reset_world()\n",
    "        return self.get_state_and_mask()\n",
    "\n",
    "    def get_state_and_mask(self):\n",
    "        return self.state, self.get_action_mask()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        prev = {\n",
    "            \"sun_before_action\": self.get_sun(),\n",
    "            \"zombie_count\": self.zombie_count,\n",
    "            \"plant_count\": self.plant_count,\n",
    "            \"brain_count\": len(self.brains),\n",
    "            \"sun_spent_before_action\": self.sun_spent,\n",
    "            # \"state\": self.state,\n",
    "        }\n",
    "\n",
    "        self._take_action(action)\n",
    "        prev[\"sun_after_action\"] = self.get_sun()\n",
    "\n",
    "        for _ in range(self.step_length):\n",
    "            self.world.update()\n",
    "        self._update_state()\n",
    "\n",
    "        game_status = self._get_game_status()\n",
    "        return (\n",
    "            self._get_reward(prev, action, game_status),\n",
    "            self.state,\n",
    "            self.get_action_mask(),\n",
    "            game_status,\n",
    "        )\n",
    "\n",
    "    def get_valid_actions(self, action_mask):\n",
    "        actions = np.arange(ACTION_SIZE)\n",
    "        return actions[action_mask]\n",
    "\n",
    "    def get_action_mask(self):\n",
    "        sun = self.get_sun()\n",
    "        mask = np.zeros(ACTION_SIZE, dtype=bool)\n",
    "        if self.zombie_count > 0:\n",
    "            mask[0] = True\n",
    "        if sun >= 50:\n",
    "            mask[1:26] = True\n",
    "        if sun >= 125:\n",
    "            mask[26:51] = True\n",
    "        if sun >= 175:\n",
    "            mask[51:] = True\n",
    "        return mask\n",
    "\n",
    "    def get_sun(self):\n",
    "        return self.world.scene.sun.sun\n",
    "\n",
    "    def _get_game_status(self):\n",
    "        if self.max_step is not None and self.step_count >= self.max_step:\n",
    "            return GameStatus.TIMEUP\n",
    "        if len(self.brains) == 0:\n",
    "            return GameStatus.WIN\n",
    "        if self.get_sun() < 50 and self.zombie_count == 0:\n",
    "            return GameStatus.LOSE\n",
    "        return GameStatus.CONTINUE\n",
    "\n",
    "    def _get_reward_pen_for_sun(self, prev, game_status):\n",
    "        if game_status == GameStatus.LOSE:\n",
    "            return -72\n",
    "\n",
    "        earned_sun = self.get_sun() - prev[\"sun_after_action\"]\n",
    "        spent_sun = prev[\"sun_before_action\"] - prev[\"sun_after_action\"]\n",
    "\n",
    "        reward = (\n",
    "            earned_sun - spent_sun * (1.001 ** prev[\"sun_spent_before_action\"])\n",
    "        ) / 25\n",
    "        if game_status == GameStatus.WIN:\n",
    "            reward += self.get_sun() / 25\n",
    "        return reward\n",
    "\n",
    "    def _get_reward_plain(self, prev, game_status):\n",
    "        # if game_status == GameStatus.LOSE:\n",
    "        #     return 0\n",
    "        return (self.get_sun() - prev[\"sun_before_action\"]) / 25\n",
    "\n",
    "    def _get_reward(self, prev, action, game_status):\n",
    "        # return self._get_reward_plain(prev, game_status)\n",
    "\n",
    "        # if game_status == GameStatus.LOSE:\n",
    "        #     return -72\n",
    "        # prev_state = prev[\"state\"]\n",
    "        eaten_plant_num = prev[\"plant_count\"] - self.plant_count\n",
    "        eaten_brain_num = prev[\"brain_count\"] - len(self.brains)\n",
    "        # zombie_count = self.zombie_count\n",
    "        reward = (self.get_sun() - prev[\"sun_before_action\"]) / 25\n",
    "        reward += eaten_brain_num * 8\n",
    "        reward += eaten_plant_num * 2\n",
    "        # for i in range(NUM_ZOMBIES):\n",
    "        #     base = i * ZOMBIE_SIZE\n",
    "        #     if self.state[base] - prev_state[base] > 0:\n",
    "        #       plant_count = 0\n",
    "        #       row = int(round(self.state[base + 2] * 5))\n",
    "        #       for j in range(NUM_PLANTS):\n",
    "        #         if self.state[base] != 0:\n",
    "        #           plant_row = int(round(self.state[base + 2] * 5))\n",
    "        #           if row == plant_row:\n",
    "        #             plant_count+=1\n",
    "        #       if plant_count <= 2:\n",
    "        #         reward -= (2-plant_count) * self.state[base]\n",
    "        if game_status == GameStatus.LOSE:\n",
    "            reward -= 5\n",
    "        return reward\n",
    "\n",
    "    def _reset_world(self) -> None:\n",
    "        self.world = World(SceneType.night)\n",
    "        self.world.scene.stop_spawn = True\n",
    "        self.world.scene.is_iz = True\n",
    "        self.world.scene.set_sun(150)\n",
    "        plant_list = [\n",
    "            plant for plant, count in plant_counts.items() for _ in range(count)\n",
    "        ]\n",
    "        np.random.shuffle(plant_list)\n",
    "        for index, plant in enumerate(plant_list):\n",
    "            self.world.plant_factory.create(\n",
    "                plant,\n",
    "                index // P_LANE_LENGTH,\n",
    "                index % P_LANE_LENGTH,\n",
    "            )\n",
    "        self._update_state()\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        if action > 0:\n",
    "            action -= 1\n",
    "            z_idx = action//(N_LANES * Z_LANE_LENGTH)\n",
    "            action_area = action%(N_LANES * Z_LANE_LENGTH)\n",
    "            row = action_area//N_LANES\n",
    "            col = action_area % N_LANES + 4\n",
    "            sun = self.get_sun() - zombie_deck[z_idx][1]\n",
    "            assert sun >= 0\n",
    "            self.world.zombie_factory.create(zombie_deck[z_idx][0], row, col)\n",
    "            self.world.scene.set_sun(sun)\n",
    "\n",
    "    def _update_state(self):\n",
    "        self.state, self.zombie_count, self.plant_count = self.ob_factory.create(\n",
    "            self.world\n",
    "        )\n",
    "        self.state.append(self.sun_spent / 1950)\n",
    "\n",
    "        self.brains = []\n",
    "        for i, b in enumerate(self.state[BRAIN_BASE : BRAIN_BASE + 5]):\n",
    "            if b > 0.5:\n",
    "                self.brains.append(i)\n",
    "\n",
    "    def print_human_readable_state(self, highlight=None):\n",
    "        def plant_str(plant_type):\n",
    "            if plant_type == 1:\n",
    "                return \"sun\"\n",
    "            if plant_type == 2:\n",
    "                return \"pea\"\n",
    "            if plant_type == 3:\n",
    "                return \"sqa\"\n",
    "            if plant_type == 4:\n",
    "                return \"sno\"\n",
    "            return \"---\"\n",
    "\n",
    "        def zombie_x_to_col(x):\n",
    "            col = int((x + 40) // 80)\n",
    "            return min(max(col, 0), LANE_LENGTH - 1)\n",
    "\n",
    "        def zombie_str(zombie_type):\n",
    "            if zombie_type == 1:\n",
    "                return \"Z\"\n",
    "            if zombie_type == 2:\n",
    "                return \"B\"\n",
    "            if zombie_type == 3:\n",
    "                return \"F\"\n",
    "            return \".\"\n",
    "\n",
    "        def acc1_hp_max(zombie_type):\n",
    "            if zombie_type == 2:\n",
    "                return 1100\n",
    "            if zombie_type == 3:\n",
    "                return 1400\n",
    "            return 0\n",
    "\n",
    "        plant_hps = [[0 for _ in range(P_LANE_LENGTH)] for _ in range(N_LANES)]\n",
    "        plant_types = [[\"---\" for _ in range(P_LANE_LENGTH)] for _ in range(N_LANES)]\n",
    "        zombie_hps = [[0 for _ in range(LANE_LENGTH)] for _ in range(N_LANES)]\n",
    "        zombie_types = [[\".\" for _ in range(LANE_LENGTH)] for _ in range(N_LANES)]\n",
    "\n",
    "        state = self.state\n",
    "\n",
    "        for i in range(NUM_PLANTS):\n",
    "            base = NUM_ZOMBIES * ZOMBIE_SIZE + i * PLANT_SIZE\n",
    "            if state[base] != 0:\n",
    "                plant_type = int(round(state[base] * 4))\n",
    "                hp = state[base + 1]\n",
    "                row = int(round(state[base + 2] * 5))\n",
    "                col = int(round(state[base + 3] * 9))\n",
    "\n",
    "                plant_hps[row][col] += hp * 300\n",
    "                plant_types[row][col] = plant_str(plant_type)\n",
    "\n",
    "        for i in range(NUM_ZOMBIES):\n",
    "            base = i * ZOMBIE_SIZE\n",
    "            if state[base] != 0:\n",
    "                zombie_type = int(round(state[base] * 3))\n",
    "                x = state[base + 1] * 650\n",
    "                row = int(round(state[base + 2] * 5))\n",
    "                hp = state[base + 3]\n",
    "                acc1_hp = state[base + 4]\n",
    "                col = zombie_x_to_col(x)\n",
    "\n",
    "                zombie_hps[row][col] += hp * 270 + acc1_hp * acc1_hp_max(zombie_type)\n",
    "                zombie_types[row][col] = zombie_str(zombie_type)\n",
    "\n",
    "        print(\"==Plant HP==\")\n",
    "        for row in range(N_LANES):\n",
    "            print(f\"row {row+1}: \", end=\"\")\n",
    "            for col in range(P_LANE_LENGTH):\n",
    "                print(f\"{plant_hps[row][col]:.2f}\\t\", end=\"\")\n",
    "            print()\n",
    "\n",
    "        print(\"==Plant Type==\")\n",
    "        for row in range(N_LANES):\n",
    "            print(f\"row {row+1}: \", end=\"\")\n",
    "            for col in range(P_LANE_LENGTH):\n",
    "                print(f\"{plant_types[row][col]}\\t\", end=\"\")\n",
    "            print()\n",
    "\n",
    "        print(\"==Zombie HP==\")\n",
    "        for row in range(N_LANES):\n",
    "            print(f\"row {row+1}: \", end=\"\")\n",
    "            for col in range(LANE_LENGTH):\n",
    "                print(f\"{zombie_hps[row][col]:.2f}\\t\", end=\"\")\n",
    "            print()\n",
    "\n",
    "        print(\"==Zombie Type==\")\n",
    "        highlight_row, highlight_col = (-1, -1) if highlight is None else highlight\n",
    "        for row in range(N_LANES):\n",
    "            print(f\"row {row+1}: \", end=\"\")\n",
    "            for col in range(LANE_LENGTH):\n",
    "                out = f\"{zombie_types[row][col]}\"\n",
    "                if row == highlight_row and col == highlight_col:\n",
    "                    out = f\"[{out}]\"\n",
    "                out += \"\\t\"\n",
    "                print(out, end=\"\")\n",
    "            print()\n",
    "\n",
    "        print(\n",
    "            f\"Step: {self.step_count}; Sun: {self.get_sun()}; Brains: {len(self.brains)}; Game status: {self._get_game_status().name} \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwuPV_EGVr1U"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3Mk7Kq1V3v-"
   },
   "source": [
    "Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gGV02Lfx4TqN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "class Epsilons:\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length,\n",
    "        start_epsilon,\n",
    "        end_epsilon,\n",
    "        interpolation=\"exponential\",\n",
    "    ):\n",
    "        self.seq_length = seq_length\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.end_epsilon = end_epsilon\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        assert seq_length > 1, \"seq_length must be positive\"\n",
    "        assert interpolation in [\"exponential\"], \"not implemented\"\n",
    "\n",
    "        self.index = 0\n",
    "        self.decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        self.has_warned = False\n",
    "\n",
    "    def get(self):\n",
    "        return self.start_epsilon * self.decay_rate**self.index\n",
    "\n",
    "    def next(self):\n",
    "        self.index += 1\n",
    "        if self.index >= self.seq_length:\n",
    "            if not self.has_warned:\n",
    "                self.has_warned = True\n",
    "                warnings.warn(\n",
    "                    f\"index = {self.index} overflows for seq_length = {self.seq_length}, using index = {self.seq_length - 1}.\"\n",
    "                )\n",
    "            self.index = self.seq_length - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LufQ8NlaXHsg"
   },
   "source": [
    "Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "w7Uuf0lD4Uod",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, test_size=500, step_count=None, output_file=None):\n",
    "    agent.set_to_eval_mode()\n",
    "    game_results = []\n",
    "    steps = []\n",
    "    winning_suns = []\n",
    "\n",
    "    for test_idx in range(1, test_size + 1):\n",
    "        print(f\"\\rTesting {test_idx}/{test_size}...\", end=\"\")\n",
    "        env = IZenv()\n",
    "        state, mask = env.get_state_and_mask()\n",
    "\n",
    "        for step in range(1_000_000):\n",
    "            action = agent.get_best_q_action(state, env.get_valid_actions(mask))\n",
    "            _, next_state, next_mask, game_status = env.step(action)\n",
    "            state, mask = next_state, next_mask\n",
    "\n",
    "            if game_status != GameStatus.CONTINUE:\n",
    "                game_results.append(game_status)\n",
    "                steps.append(step)\n",
    "                if game_status == GameStatus.WIN:\n",
    "                    winning_suns.append(env.get_sun())\n",
    "                break\n",
    "\n",
    "    print()\n",
    "\n",
    "    results_counter = Counter(game_results)\n",
    "    agent.set_to_training_mode()\n",
    "    return (np.mean(winning_suns), results_counter.get(GameStatus.WIN, 0) / len(game_results))\n",
    "\n",
    "\n",
    "def manually_test_agent(agent, fix_rand=True):\n",
    "    agent.set_to_eval_mode()\n",
    "\n",
    "    if fix_rand:\n",
    "        np.random.seed(0)\n",
    "    else:\n",
    "        np.random.seed()\n",
    "\n",
    "    env = IZenv()\n",
    "    state, mask = env.get_state_and_mask()\n",
    "    last_step = 0\n",
    "\n",
    "    for step in range(10000):\n",
    "        action = agent.get_best_q_action(state, env.get_valid_actions(mask))\n",
    "        reward, next_state, next_mask, game_status = env.step(action)\n",
    "\n",
    "        if action != 0 or game_status != GameStatus.CONTINUE:\n",
    "            env.print_human_readable_state(\n",
    "                highlight=((action - 1) % 5, 4) if action != 0 else None\n",
    "            )\n",
    "            print(f\"Action: {action}, Reward: {reward}, Î”Step: {step - last_step}\")\n",
    "            last_step = step\n",
    "            _ = input(\"\")\n",
    "\n",
    "        state, mask = next_state, next_mask\n",
    "\n",
    "        if game_status != GameStatus.CONTINUE:\n",
    "            break\n",
    "\n",
    "    agent.set_to_training_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1b2cJvFXSyy"
   },
   "source": [
    "Noisy Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cd0wnuOBXSYt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear module for NoisyNet.\n",
    "\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "        in_features (int): input size of linear module\n",
    "        out_features (int): output size of linear module\n",
    "        std_init (float): initial std value\n",
    "        weight_mu (nn.Parameter): mean value weight parameter\n",
    "        weight_sigma (nn.Parameter): std value weight parameter\n",
    "        bias_mu (nn.Parameter): mean value bias parameter\n",
    "        bias_sigma (nn.Parameter): std value bias parameter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        std_init: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(NoisyLinear, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.Tensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Make new noise.\"\"\"\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "\n",
    "        # outer product\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\n",
    "\n",
    "        We don't use separate statements on train / eval mode.\n",
    "        It doesn't show remarkable difference of performance.\n",
    "        \"\"\"\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_noise(size: int) -> torch.Tensor:\n",
    "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
    "        x = torch.randn(size)\n",
    "\n",
    "        return x.sign().mul(x.abs().sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRsEA7Y7XkiN"
   },
   "source": [
    "Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_O92wAQaXnSz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
    "\n",
    "\n",
    "def create_folder_if_not_exist(folder_name):\n",
    "    current_directory = os.getcwd()\n",
    "    folder_path = os.path.join(current_directory, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "\n",
    "def format_num(n):\n",
    "    if n > 1_000_000:\n",
    "        if n % 1_000_000 == 0:\n",
    "            return f\"{int(n / 1_000_000)}m\"\n",
    "        return f\"{n / 1_000_000}m\"\n",
    "    if n > 1_000:\n",
    "        if n % 1_000 == 0:\n",
    "            return f\"{int(n / 1_000)}k\"\n",
    "        return f\"{n / 1_000}k\"\n",
    "    return str(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fbiTTYKXqUT"
   },
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5Gk1qbGFXtxz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 23 15:25:15 2021\n",
    "\n",
    "@author: Lukas Frank\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "class Threshold:\n",
    "    \"\"\"\n",
    "    Generate sequences of epsilon thresholds.\n",
    "\n",
    "    :param seq_length: int, length of epsilon sequence = number of epsilons to draw\n",
    "\n",
    "    :param start_epsilon: float, value to start with\n",
    "\n",
    "    :param end_epsilon (optional): float, value to end with. If None, return constant\n",
    "        sequence of value start_epsilon. Default: None.\n",
    "\n",
    "    :param interpolation (optional): string, interpolation method:\\n\n",
    "        either 'linear', 'exponential' or 'sinusoidal'. Default: 'linear'.\n",
    "        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf\n",
    "\n",
    "    :param periods: int, number of periods for sinusoidal sequence. Default: 10.\n",
    "        ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length,\n",
    "        start_epsilon,\n",
    "        end_epsilon=None,\n",
    "        interpolation=\"linear\",\n",
    "        periods=10,\n",
    "    ):\n",
    "        self.seq_length = seq_length\n",
    "        self.start_epsilon = start_epsilon\n",
    "        assert interpolation in [\n",
    "            \"linear\",\n",
    "            \"exponential\",\n",
    "            \"sinusoidal\",\n",
    "        ], \"interpolation argument invalid. Must be 'linear', 'exponential', 'sinusoidal' or unspecified.\"\n",
    "        self.interpolation = interpolation\n",
    "        if end_epsilon is None:\n",
    "            self.end_epsilon = start_epsilon\n",
    "            # set to linear to deliver constant sequence of epsilons.\n",
    "            self.interpolation = \"linear\"\n",
    "        else:\n",
    "            self.end_epsilon = end_epsilon\n",
    "        self.periods = periods\n",
    "\n",
    "    def epsilon(self, index=None):\n",
    "        \"\"\"Return sequence or element of sequence of epsilons as specified\n",
    "\n",
    "        :param index (optional): index of sequence element to be returned. If None, return\n",
    "            full sequence. Default: None.\\n\n",
    "\n",
    "        :return: array-like with shape (self.seq_length) or a single float value.\n",
    "        \"\"\"\n",
    "        epsilon = None\n",
    "\n",
    "        if self.interpolation == \"linear\":\n",
    "            epsilon = self._linear(index)\n",
    "\n",
    "        elif self.interpolation == \"exponential\":\n",
    "            epsilon = self._exponential(index)\n",
    "\n",
    "        elif self.interpolation == \"sinusoidal\":\n",
    "            epsilon = self._sinusoidal(index, self.periods)\n",
    "\n",
    "        return epsilon\n",
    "\n",
    "    def _linear(self, index):\n",
    "        \"\"\"Calls linear calculation method depending on whether index is given or not.\"\"\"\n",
    "        if index is not None:  # return only one epsilon\n",
    "            self._check_index_length(index)\n",
    "            return self._linear_point(index)\n",
    "        else:\n",
    "            return self._linear_sequence()\n",
    "\n",
    "    def _exponential(self, index):\n",
    "        \"\"\"Calls exponential calculation depending on whether index is given or not.\"\"\"\n",
    "        if index is not None:  # return only one epsilon\n",
    "            self._check_index_length(index)\n",
    "            return self._exponential_point(index)\n",
    "        else:\n",
    "            return self._exponential_sequence()\n",
    "\n",
    "    def _sinusoidal(self, index, periods):\n",
    "        \"\"\"Calls sinusoidal calculation depending on whether index is given or not.\"\"\"\n",
    "        if index is not None:  # return only one epsilon\n",
    "            self._check_index_length(index)\n",
    "            return self._sinusoidal_point(index, mini_epochs=periods)\n",
    "        else:\n",
    "            return self._sinusoidal_sequence(mini_epochs=periods)\n",
    "\n",
    "    def _linear_sequence(self):\n",
    "        \"\"\"Computes linear sequence\"\"\"\n",
    "        return np.linspace(\n",
    "            start=self.start_epsilon, stop=self.end_epsilon, num=self.seq_length\n",
    "        ).tolist()\n",
    "\n",
    "    def _linear_point(self, index):\n",
    "        \"\"\"Computes a single point by linear interpolation\"\"\"\n",
    "        return (\n",
    "            self.start_epsilon\n",
    "            + (self.end_epsilon - self.start_epsilon) / (self.seq_length - 1) * index\n",
    "        )\n",
    "\n",
    "    def _exponential_sequence(self):\n",
    "        \"\"\"Computes exponential sequence\"\"\"\n",
    "        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        return [(self.start_epsilon * decay_rate**i) for i in range(self.seq_length)]\n",
    "\n",
    "    def _exponential_point(self, index):\n",
    "        \"\"\"Computes a single point by exponential interpolation\"\"\"\n",
    "        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        return self.start_epsilon * decay_rate**index\n",
    "\n",
    "    def _sinusoidal_sequence(self, mini_epochs):\n",
    "        \"\"\"Computes sinusoidal sequence.\n",
    "\n",
    "        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf \\n\n",
    "\n",
    "        :param mini_epochs (optional): int, number of oscillations in sequence.\n",
    "        \"\"\"\n",
    "        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        return [\n",
    "            (\n",
    "                self.start_epsilon\n",
    "                * decay_rate**i\n",
    "                * 0.5\n",
    "                * (1 + np.cos(2 * math.pi * i * mini_epochs / (self.seq_length - 1)))\n",
    "            )\n",
    "            for i in range(self.seq_length)\n",
    "        ]\n",
    "\n",
    "    def _sinusoidal_point(self, index, mini_epochs):\n",
    "        \"\"\"Computes a single point by sinusoidal interpolation.\n",
    "\n",
    "        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf \\n\n",
    "\n",
    "        :param mini_epochs (optional): int, number of oscillations in sequence.\n",
    "        \"\"\"\n",
    "        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        return (\n",
    "            self.start_epsilon\n",
    "            * decay_rate**index\n",
    "            * 0.5\n",
    "            * (1 + np.cos(2 * math.pi * index * mini_epochs / (self.seq_length - 1)))\n",
    "        )\n",
    "\n",
    "    def _check_index_length(self, index):\n",
    "        \"\"\"Check whether index is in sequence.\"\"\"\n",
    "        if index >= self.seq_length:\n",
    "            warnings.warn(\n",
    "                f\"threshold.epsilon(index): index {index} > seq_length {self.seq_length}.\"\n",
    "                f\"Changing index to {index - self.seq_length}.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RHmcNTjX5er"
   },
   "source": [
    "Segment Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F4d14OvUX7nk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Segment tree for Prioritized Replay Buffer.\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class SegmentTree:\n",
    "    \"\"\" Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int)\n",
    "        tree (list)\n",
    "        operation (function)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, operation: Callable, init_value: float):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "            operation (function)\n",
    "            init_value (float)\n",
    "\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            capacity > 0 and capacity & (capacity - 1) == 0\n",
    "        ), \"capacity must be positive and a power of 2.\"\n",
    "        self.capacity = capacity\n",
    "        self.tree = [init_value for _ in range(2 * capacity)]\n",
    "        self.operation = operation\n",
    "\n",
    "    def _operate_helper(\n",
    "        self, start: int, end: int, node: int, node_start: int, node_end: int\n",
    "    ) -> float:\n",
    "        \"\"\"Returns result of operation in segment.\"\"\"\n",
    "        if start == node_start and end == node_end:\n",
    "            return self.tree[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._operate_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._operate_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self.operation(\n",
    "                    self._operate_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._operate_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),\n",
    "                )\n",
    "\n",
    "    def operate(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns result of applying `self.operation`.\"\"\"\n",
    "        if end <= 0:\n",
    "            end += self.capacity\n",
    "        end -= 1\n",
    "\n",
    "        return self._operate_helper(start, end, 1, 0, self.capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx: int, val: float):\n",
    "        \"\"\"Set value in tree.\"\"\"\n",
    "        idx += self.capacity\n",
    "        self.tree[idx] = val\n",
    "\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self.tree[idx] = self.operation(self.tree[2 * idx], self.tree[2 * idx + 1])\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx: int) -> float:\n",
    "        \"\"\"Get real value in leaf node of tree.\"\"\"\n",
    "        assert 0 <= idx < self.capacity\n",
    "\n",
    "        return self.tree[self.capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    \"\"\" Create SumSegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=operator.add, init_value=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns arr[start] + ... + arr[end].\"\"\"\n",
    "        return super(SumSegmentTree, self).operate(start, end)\n",
    "\n",
    "    def retrieve(self, upperbound: float) -> int:\n",
    "        \"\"\"Find the highest index `i` about upper bound in the tree\"\"\"\n",
    "        # TODO: Check assert case and fix bug\n",
    "        assert 0 <= upperbound <= self.sum() + 1e-5, \"upperbound: {}\".format(upperbound)\n",
    "\n",
    "        idx = 1\n",
    "\n",
    "        while idx < self.capacity:  # while non-leaf\n",
    "            left = 2 * idx\n",
    "            right = left + 1\n",
    "            if self.tree[left] > upperbound:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                upperbound -= self.tree[left]\n",
    "                idx = right\n",
    "        return idx - self.capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    \"\"\" Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=min, init_value=float(\"inf\")\n",
    "        )\n",
    "\n",
    "    def min(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end]).\"\"\"\n",
    "        return super(MinSegmentTree, self).operate(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-SJbQ7VX_jh"
   },
   "source": [
    "Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CC4-hF7bYB8K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import Tuple, Dict, Deque, List\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        size: int,\n",
    "        batch_size: int = 32,\n",
    "        n_step: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        (\n",
    "            self.ptr,\n",
    "            self.size,\n",
    "        ) = (\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        # for N-step Learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: np.ndarray,\n",
    "        rew: float,\n",
    "        next_obs: np.ndarray,\n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        transition = (obs, act, rew, next_obs, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        # single step transition is not ready\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return ()\n",
    "\n",
    "        # make a n-step transition\n",
    "        rew, next_obs, done = self._get_n_step_info(self.n_step_buffer, self.gamma)\n",
    "        obs, act = self.n_step_buffer[0][:2]\n",
    "\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        return self.n_step_buffer[0]\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "            # for N-step Learning\n",
    "            indices=idxs,\n",
    "        )\n",
    "\n",
    "    def sample_batch_from_idxs(self, idxs: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        # for N-step Learning\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "        )\n",
    "\n",
    "    def _get_n_step_info(\n",
    "        self, n_step_buffer: Deque, gamma: float\n",
    "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
    "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
    "        # info of the last transition\n",
    "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
    "\n",
    "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
    "            r, n_o, d = transition[-3:]\n",
    "\n",
    "            rew = r + gamma * rew * (1 - d)\n",
    "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
    "\n",
    "        return rew, next_obs, done\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    \"\"\"Prioritized Replay buffer.\n",
    "\n",
    "    Attributes:\n",
    "        max_priority (float): max priority\n",
    "        tree_ptr (int): next index of tree\n",
    "        alpha (float): alpha parameter for prioritized replay buffer\n",
    "        sum_tree (SumSegmentTree): sum tree for prior\n",
    "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        size: int,\n",
    "        batch_size: int = 32,\n",
    "        alpha: float = 0.6,\n",
    "        n_step: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        assert alpha >= 0\n",
    "\n",
    "        super(PrioritizedReplayBuffer, self).__init__(\n",
    "            obs_dim, size, batch_size, n_step, gamma\n",
    "        )\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: int,\n",
    "        rew: float,\n",
    "        next_obs: np.ndarray,\n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        \"\"\"Store experience and priority.\"\"\"\n",
    "        transition = super().store(obs, act, rew, next_obs, done)\n",
    "\n",
    "        if transition:\n",
    "            self.sum_tree[self.tree_ptr] = self.max_priority**self.alpha\n",
    "            self.min_tree[self.tree_ptr] = self.max_priority**self.alpha\n",
    "            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "\n",
    "        return transition\n",
    "\n",
    "    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "\n",
    "        indices = self._sample_proportional()\n",
    "\n",
    "        obs = self.obs_buf[indices]\n",
    "        next_obs = self.next_obs_buf[indices]\n",
    "        acts = self.acts_buf[indices]\n",
    "        rews = self.rews_buf[indices]\n",
    "        done = self.done_buf[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "\n",
    "        return dict(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            acts=acts,\n",
    "            rews=rews,\n",
    "            done=done,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
    "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority**self.alpha\n",
    "            self.min_tree[idx] = priority**self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        \"\"\"Sample indices based on proportions.\"\"\"\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment = p_total / self.batch_size\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def _calculate_weight(self, idx: int, beta: float):\n",
    "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "\n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IWB1PI1UQv54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim: int, out_dim: int, atom_size: int, support: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.support = support\n",
    "        self.out_dim = out_dim\n",
    "        self.atom_size = atom_size\n",
    "\n",
    "        # set common feature layer\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # set advantage layer\n",
    "        self.advantage_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.advantage_layer = NoisyLinear(128, out_dim * atom_size)\n",
    "\n",
    "        # set value layer\n",
    "        self.value_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.value_layer = NoisyLinear(128, atom_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        dist = self.dist(x)\n",
    "        q = torch.sum(dist * self.support, dim=2)\n",
    "\n",
    "        return q\n",
    "\n",
    "    def dist(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get distribution for atoms.\"\"\"\n",
    "        feature = self.feature_layer(x)\n",
    "        adv_hid = F.relu(self.advantage_hidden_layer(feature))\n",
    "        val_hid = F.relu(self.value_hidden_layer(feature))\n",
    "\n",
    "        advantage = self.advantage_layer(adv_hid).view(-1, self.out_dim, self.atom_size)\n",
    "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "        dist = dist.clamp(min=1e-3)  # for avoiding nans\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset all noisy layers.\"\"\"\n",
    "        self.advantage_hidden_layer.reset_noise()\n",
    "        self.advantage_layer.reset_noise()\n",
    "        self.value_hidden_layer.reset_noise()\n",
    "        self.value_layer.reset_noise()\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "\n",
    "    Attribute:\n",
    "        env (IZenv): izombie env\n",
    "        memory (PrioritizedReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including\n",
    "                           state, action, reward, next_state, done\n",
    "        v_min (float): min value of support\n",
    "        v_max (float): max value of support\n",
    "        atom_size (int): the unit number of support\n",
    "        support (torch.Tensor): support for categorical dqn\n",
    "        use_n_step (bool): whether to use n_step memory\n",
    "        n_step (int): step number to calculate n-step td error\n",
    "        memory_n (ReplayBuffer): n-step replay buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: IZenv,\n",
    "        model_name: str,\n",
    "        device: str,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-3,\n",
    "        # PER parameters\n",
    "        alpha: float = 0.2,\n",
    "        beta: float = 0.6,\n",
    "        prior_eps: float = 1e-6,\n",
    "        # Categorical DQN parameters\n",
    "        v_min: float = 0.0,\n",
    "        v_max: float = 200.0,\n",
    "        atom_size: int = 51,\n",
    "        # N-step Learning\n",
    "        n_step: int = 3,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            lr (float): learning rate\n",
    "            gamma (float): discount factor\n",
    "            alpha (float): determines how much prioritization is used\n",
    "            beta (float): determines how much importance sampling is used\n",
    "            prior_eps (float): guarantees every transition can be sampled\n",
    "            v_min (float): min value of support\n",
    "            v_max (float): max value of support\n",
    "            atom_size (int): the unit number of support\n",
    "            n_step (int): step number to calculate n-step td error\n",
    "        \"\"\"\n",
    "        if atom_size is None:\n",
    "            atom_size = v_max - v_min + 1\n",
    "        obs_dim = STATE_SIZE\n",
    "        action_dim = ACTION_SIZE\n",
    "\n",
    "        self.env = env\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        # NoisyNet: All attributes related to epsilon are removed\n",
    "\n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(device)\n",
    "        print(f\"Using {self.device} device.\")\n",
    "\n",
    "        # PER\n",
    "        # memory for 1-step Learning\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            obs_dim, memory_size, batch_size, alpha=alpha, gamma=gamma\n",
    "        )\n",
    "\n",
    "        # memory for N-step Learning\n",
    "        self.use_n_step = True if n_step > 1 else False\n",
    "        if self.use_n_step:\n",
    "            self.n_step = n_step\n",
    "            self.memory_n = ReplayBuffer(\n",
    "                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n",
    "            )\n",
    "\n",
    "        # Categorical DQN parameters\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.atom_size = atom_size\n",
    "        self.support = torch.linspace(self.v_min, self.v_max, self.atom_size).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(obs_dim, action_dim, self.atom_size, self.support).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.dqn_target = Network(obs_dim, action_dim, self.atom_size, self.support).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "\n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "        # stats\n",
    "        self.winning_suns = []\n",
    "        self.losses = []\n",
    "        self.game_results = []\n",
    "        self.steps = []\n",
    "        self.scores = []\n",
    "        self.stats_data = []\n",
    "\n",
    "    def get_best_q_action(self, state, mask):\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # NoisyNet: no epsilon greedy action selection\n",
    "        with torch.no_grad():\n",
    "            valid_actions = self.env.get_valid_actions(mask)\n",
    "            q_values = self.dqn(\n",
    "                torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            ).detach()\n",
    "            valid_q_values = q_values[0, valid_actions]\n",
    "            max_q_index = torch.argmax(valid_q_values).item()\n",
    "            selected_action = valid_actions[max_q_index]\n",
    "\n",
    "            if not self.is_test:\n",
    "                self.transition = [state, selected_action]\n",
    "\n",
    "            return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        reward, next_state, next_mask, game_status = self.env.step(action)\n",
    "        done = game_status != GameStatus.CONTINUE\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "\n",
    "            # N-step transition\n",
    "            if self.use_n_step:\n",
    "                one_step_transition = self.memory_n.store(*self.transition)\n",
    "            # 1-step transition\n",
    "            else:\n",
    "                one_step_transition = self.transition\n",
    "\n",
    "            # add a single step transition\n",
    "            if one_step_transition:\n",
    "                self.memory.store(*one_step_transition)\n",
    "\n",
    "        return next_state, next_mask, reward, game_status, done\n",
    "\n",
    "    def update_main_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        # PER needs beta to calculate weights\n",
    "        samples = self.memory.sample_batch(self.beta)\n",
    "        weights = torch.FloatTensor(samples[\"weights\"].reshape(-1, 1)).to(self.device)\n",
    "        indices = samples[\"indices\"]\n",
    "\n",
    "        # 1-step Learning loss\n",
    "        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n",
    "\n",
    "        # PER: importance sampling before average\n",
    "        loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        # N-step Learning loss\n",
    "        # we are gonna combine 1-step loss and n-step loss so as to\n",
    "        # prevent high-variance. The original rainbow employs n-step loss only.\n",
    "        if self.use_n_step:\n",
    "            gamma = self.gamma**self.n_step\n",
    "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
    "            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n",
    "            elementwise_loss += elementwise_loss_n_loss\n",
    "\n",
    "            # PER: importance sampling before average\n",
    "            loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.dqn.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # PER: update priorities\n",
    "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
    "        new_priorities = loss_for_prior + self.prior_eps\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "\n",
    "        # NoisyNet: reset noise\n",
    "        self.dqn.reset_noise()\n",
    "        self.dqn_target.reset_noise()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_steps: int,\n",
    "        stats_window: int = 1_000,\n",
    "        print_stats_every=30_000,\n",
    "        update_target_every=2000,\n",
    "        update_main_every=1,\n",
    "        save_every=None,\n",
    "        eval_every=None,\n",
    "    ):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        model_dir = f\"model/{self.model_name}_{get_timestamp()}\"\n",
    "        self.is_test = False\n",
    "        self.set_to_training_mode()\n",
    "\n",
    "        state, mask = self.env.reset()\n",
    "        start_time = datetime.datetime.now()\n",
    "        score = 0\n",
    "\n",
    "        for step_idx in range(1, num_steps + 1):\n",
    "            action = self.get_best_q_action(state, mask)\n",
    "            state, mask, reward, game_status, done = self.step(action)\n",
    "            score += reward\n",
    "\n",
    "            # NoisyNet: removed decrease of epsilon\n",
    "\n",
    "            # PER: increase beta\n",
    "            fraction = min(step_idx / num_steps, 1.0)\n",
    "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                self.game_results.append(game_status)\n",
    "                self.steps.append(self.env.step_count)\n",
    "                self.scores.append(score)\n",
    "                score = 0\n",
    "                if game_status == GameStatus.WIN:\n",
    "                    self.winning_suns.append(self.env.get_sun())\n",
    "                state, mask = self.env.reset()\n",
    "\n",
    "            # if training is ready\n",
    "            if (\n",
    "                len(self.memory) >= self.batch_size\n",
    "                and step_idx % update_main_every == 0\n",
    "            ):\n",
    "                loss = self.update_main_model()\n",
    "                self.losses.append(loss)\n",
    "\n",
    "            # if hard update is needed\n",
    "            if step_idx % update_target_every == 0:\n",
    "                self._sync_target_with_main()\n",
    "\n",
    "            if step_idx % print_stats_every == 0:\n",
    "                self.print_stats(stats_window, step_idx, num_steps, start_time)\n",
    "\n",
    "            if eval_every is not None and step_idx % eval_every == 0:\n",
    "                avg_suns, win_rate = evaluate_agent(self,step_count=step_idx)\n",
    "                self.stats_data.append((step_idx, win_rate, avg_suns))\n",
    "            if (\n",
    "                save_every is not None and step_idx % save_every == 0\n",
    "            ) or step_idx == num_steps:\n",
    "                self.save(f\"{model_dir}/{format_num(step_idx)}.pth\")\n",
    "\n",
    "    def _compute_dqn_loss(\n",
    "        self, samples: Dict[str, np.ndarray], gamma: float\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Return categorical dqn loss.\"\"\"\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(self.device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(self.device)\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(self.device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(self.device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(self.device)\n",
    "\n",
    "        # Categorical DQN algorithm\n",
    "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_action = self.dqn(next_state).argmax(1)\n",
    "            next_dist = self.dqn_target.dist(next_state)\n",
    "            next_dist = next_dist[range(self.batch_size), next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n",
    "                )\n",
    "                .long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.atom_size)\n",
    "                .to(self.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn.dist(state)\n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "\n",
    "        return elementwise_loss\n",
    "\n",
    "    def _sync_target_with_main(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def print_stats(self, stats_window, curr_step, total_step, start_time):\n",
    "        win_rate = (\n",
    "            sum(1 for res in self.game_results[-stats_window:] if res == GameStatus.WIN)\n",
    "            * 100\n",
    "            / min(stats_window, len(self.game_results))\n",
    "        )\n",
    "        elasped_seconds = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        print(\n",
    "            f\"Sp {format_num(curr_step)}/{format_num(total_step)} \"\n",
    "            f\"Mean losses {np.mean(self.losses[-stats_window:]):.2f} \"\n",
    "            f\"Mean winning sun {np.mean(self.winning_suns[-stats_window:]):.2f} \"\n",
    "            f\"Mean steps {np.mean(self.steps[-stats_window:]):.2f} \"\n",
    "            f\"Mean score {np.mean(self.scores[-stats_window:]):.2f} \"\n",
    "            f\"Win {win_rate:.2f}% \"\n",
    "            f\"{elasped_seconds / curr_step * 1_000_000:.2f}s/1m steps\"\n",
    "        )\n",
    "\n",
    "    def set_to_training_mode(self):\n",
    "        self.dqn.train()\n",
    "\n",
    "    def set_to_eval_mode(self):\n",
    "        self.dqn.eval()\n",
    "\n",
    "    def save(self, filename):\n",
    "        assert not os.path.exists(filename)\n",
    "        create_folder_if_not_exist(os.path.dirname(filename))\n",
    "        torch.save(self.dqn.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=self.device)\n",
    "        self.dqn.load_state_dict(state_dict)\n",
    "        self.dqn_target.load_state_dict(state_dict)\n",
    "    \n",
    "    def save_stats_to_csv(self, filename):\n",
    "        df = pd.DataFrame(self.stats_data, columns=['Step Count', 'Winning Rate', 'Average Remaining Suns'])\n",
    "        df.to_csv(filename, index=False)\n",
    "    \n",
    "    def plot_stats_in_one_graph(self, title):\n",
    "        # Extract data\n",
    "        episodes = [data[0] for data in self.stats_data]\n",
    "        win_rates = [data[1] * 100 for data in self.stats_data]\n",
    "        avg_suns = [data[2] for data in self.stats_data]\n",
    "\n",
    "        # Create subplots\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # Plot win rate\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('Step Count')\n",
    "        ax1.set_ylabel('Winning Rate (%)', color=color)\n",
    "        ax1.plot(episodes, win_rates, label=\"Winning Rate\", color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # Create a second y-axis for average suns\n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Average Remaining Suns', color=color) \n",
    "        ax2.plot(episodes, avg_suns, label=\"Average Remaining Suns\", color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout() \n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_stats_winning_rate(self, title):\n",
    "        # Extract data\n",
    "        episodes = [data[0] for data in self.stats_data]\n",
    "        win_rates = [data[1] * 100 for data in self.stats_data]\n",
    "\n",
    "        # Plot for Win Rate\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(episodes, win_rates, label=\"Winning Rate\", color='tab:red')\n",
    "        plt.xlabel(\"Step Count\")\n",
    "        plt.ylabel(\"Winning Rate (%)\")\n",
    "        plt.title(title)\n",
    "        # plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_stats_remaining_suns(self, title):\n",
    "        # Extract data\n",
    "        episodes = [data[0] for data in self.stats_data]\n",
    "        avg_suns = [data[2] for data in self.stats_data]\n",
    "\n",
    "        # Plot for Average Remaining Suns\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(episodes, avg_suns, label=\"Average Remaining Suns\", color='tab:blue')\n",
    "        plt.xlabel(\"Step Count\")\n",
    "        plt.ylabel(\"Average Remaining Suns\")\n",
    "        plt.title(title)\n",
    "        # plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbXmAC1IYzb7"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3YEpc2RLQxyn",
    "outputId": "86718efd-1c54-41ff-8c2b-037ccb0f2521",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    def seed_torch(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.backends.cudnn.enabled:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    seed_torch(seed)\n",
    "\n",
    "\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "env = IZenv()\n",
    "\n",
    "num_steps = 6_000_000\n",
    "agent = DQNAgent(\n",
    "    env,\n",
    "    device=\"cuda\",\n",
    "    model_name=\"r2\",\n",
    "    memory_size=1_000_000,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    alpha=0.2,\n",
    "    beta=0.6,\n",
    "    prior_eps=1e-6,\n",
    "    v_min=-11,\n",
    "    v_max=80,\n",
    "    atom_size=None,\n",
    "    n_step=3,\n",
    "    lr=1e-3,\n",
    ")\n",
    "# agent.load(\"model/r1_2023.11.19_00.55.17/42.5m.pth\")\n",
    "# manually_test_agent(agent, fix_rand=False)\n",
    "agent.train(\n",
    "    update_target_every=2000,\n",
    "    update_main_every=16,\n",
    "    num_steps=num_steps,\n",
    "    print_stats_every=10_000,\n",
    "    save_every=100_000,\n",
    "    eval_every=200_000,\n",
    ")\n",
    "manually_test_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.save_stats_to_csv(\"RainbowDQN(n=3bs=128).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.plot_stats_in_one_graph(\"Performance Over Steps for RainbowDQN (n=3bs=128)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_stats_winning_rate(\"Winning Rate Over Steps for RainbowDQN (n=3bs=128)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_stats_remaining_suns(\"Average Remaining Suns Over Steps for RainbowDQN (n=3bs=128)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tXwXYckvViOv",
    "ZwuPV_EGVr1U"
   ],
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1QN5f6jlyUnK01ze7FfnMPtTV3idSqfIv",
     "timestamp": 1701487140203
    },
    {
     "file_id": "1osKdj0NZ7pEPboKO8OGBY4CN9RbyIAel",
     "timestamp": 1700943976756
    },
    {
     "file_id": "1QQWUWXai-SIRGqAOD3soYMLpY7LtiE2C",
     "timestamp": 1700937785634
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
