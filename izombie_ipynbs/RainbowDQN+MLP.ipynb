{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["tXwXYckvViOv","ZwuPV_EGVr1U"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rwAQ6J6f4Gp4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700859755204,"user_tz":480,"elapsed":166681,"user":{"displayName":"Mengfei Qi","userId":"03246893569933282016"}},"outputId":"64ced705-6a93-4db5-eb70-da92fddbe29a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/Rottenham/PvZ-Emulator\n","  Cloning https://github.com/Rottenham/PvZ-Emulator to /tmp/pip-req-build-gz_cmcia\n","  Running command git clone --filter=blob:none --quiet https://github.com/Rottenham/PvZ-Emulator /tmp/pip-req-build-gz_cmcia\n","  Resolved https://github.com/Rottenham/PvZ-Emulator to commit 5d10630c2e929b53fb2ae206ae2e97caf6ed6ba7\n","  Running command git submodule update --init --recursive -q\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pvzemu\n","  Building wheel for pvzemu (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pvzemu: filename=pvzemu-0.9.0-cp310-cp310-linux_x86_64.whl size=7344998 sha256=11befde6810b8ba5d4e1ccf15ca5bca34bb1a53193570cccaddd25cae0dfd587\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9xhk6bnd/wheels/1e/c0/11/eea7b69cb73073f9c99de26f91162b5ec06042cd4fae5b2693\n","Successfully built pvzemu\n","Installing collected packages: pvzemu\n","Successfully installed pvzemu-0.9.0\n"]}],"source":["# IMPORTANT! First, go to Runtime -> Change runtime type, select a GPU runtime\n","# And then Runtime -> Run all\n","!pip install git+https://github.com/Rottenham/PvZ-Emulator"]},{"cell_type":"markdown","source":["# Env"],"metadata":{"id":"tXwXYckvViOv"}},{"cell_type":"markdown","source":["**config**"],"metadata":{"id":"6aM2hTDsUp7i"}},{"cell_type":"code","source":["from enum import Enum\n","\n","\n","class GameStatus(Enum):\n","    CONTINUE = 0\n","    WIN = 1\n","    LOSE = 2\n","    TIMEUP = 3\n","\n","\n","N_LANES = 5  # Height\n","LANE_LENGTH = 9  # Width\n","P_LANE_LENGTH = 4\n","N_PLANT_TYPE = 4\n","N_ZOMBIE_TYPE = 3\n","SUN_MAX = 1950\n","\n","# action\n","ACTION_SIZE = N_ZOMBIE_TYPE * N_LANES + 1\n","\n","# state\n","NUM_ZOMBIES = 39\n","NUM_PLANTS = 20\n","ZOMBIE_SIZE = 6\n","PLANT_SIZE = 4\n","BRAIN_BASE = NUM_ZOMBIES * ZOMBIE_SIZE + NUM_PLANTS * PLANT_SIZE + 1  # extra 1 for sun\n","BRAIN_SIZE = 5\n","STATE_SIZE = BRAIN_BASE + BRAIN_SIZE + 1"],"metadata":{"id":"O9qNPjfA4QmF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["env"],"metadata":{"id":"Rru16G2zUsKq"}},{"cell_type":"code","source":["import numpy as np\n","\n","from pvzemu import (\n","    World,\n","    SceneType,\n","    ZombieType,\n","    PlantType,\n","    IZObservation,\n",")\n","\n","plant_counts = {\n","    PlantType.sunflower: 9,\n","    PlantType.pea_shooter: 6,\n","    PlantType.squash: 3,\n","    PlantType.snow_pea: 2,\n","}\n","\n","zombie_deck = [\n","    [ZombieType.zombie, 50],\n","    [ZombieType.buckethead, 125],\n","    [ZombieType.football, 175],\n","]\n","\n","\n","class IZenv:\n","    def __init__(self, step_length=50, max_step=None):\n","        self.step_length = step_length\n","        self.max_step = max_step\n","\n","        self.ob_factory = IZObservation(NUM_ZOMBIES, NUM_PLANTS)\n","        self.state = []\n","        self.zombie_count, self.plant_count, self.brains = 0, 0, [0, 1, 2, 3, 4]\n","\n","        self.step_count = 0\n","        self.sun_spent = 0\n","        self.world = World(SceneType.night)\n","        self._reset_world()\n","\n","    def reset(self):\n","        self.zombie_count, self.plant_count, self.brains = 0, 0, [0, 1, 2, 3, 4]\n","        self.step_count = 0\n","        self.sun_spent = 0\n","        self._reset_world()\n","        return self.get_state_and_mask()\n","\n","    def get_state_and_mask(self):\n","        return self.state, self.get_action_mask()\n","\n","    def step(self, action):\n","        self.step_count += 1\n","        prev = {\n","            \"sun_before_action\": self.get_sun(),\n","            \"zombie_count\": self.zombie_count,\n","            \"plant_count\": self.plant_count,\n","            \"brain_count\": len(self.brains),\n","            \"sun_spent_before_action\": self.sun_spent,\n","        }\n","\n","        self._take_action(action)\n","        prev[\"sun_after_action\"] = self.get_sun()\n","\n","        for _ in range(self.step_length):\n","            self.world.update()\n","        self._update_state()\n","\n","        game_status = self._get_game_status()\n","        return (\n","            self._get_reward(prev, action, game_status),\n","            self.state,\n","            self.get_action_mask(),\n","            game_status,\n","        )\n","\n","    def get_valid_actions(self, action_mask):\n","        actions = np.arange(ACTION_SIZE)\n","        return actions[action_mask]\n","\n","    def get_action_mask(self):\n","        sun = self.get_sun()\n","        mask = np.zeros(ACTION_SIZE, dtype=bool)\n","        if self.zombie_count > 0:\n","            mask[0] = True\n","        if sun >= 50:\n","            mask[1:6] = True\n","        if sun >= 125:\n","            mask[6:11] = True\n","        if sun >= 175:\n","            mask[11:16] = True\n","        for row in range(5):\n","            if not row in self.brains:\n","                for i in range(3):\n","                    mask[i * 5 + row + 1] = False\n","        return mask\n","\n","    def get_sun(self):\n","        return self.world.scene.sun.sun\n","\n","    def _get_game_status(self):\n","        if self.max_step is not None and self.step_count >= self.max_step:\n","            return GameStatus.TIMEUP\n","        if len(self.brains) == 0:\n","            return GameStatus.WIN\n","        if self.get_sun() < 50 and self.zombie_count == 0:\n","            return GameStatus.LOSE\n","        return GameStatus.CONTINUE\n","\n","    def _get_reward_pen_for_sun(self, prev, game_status):\n","        if game_status == GameStatus.LOSE:\n","            return -72\n","\n","        earned_sun = self.get_sun() - prev[\"sun_after_action\"]\n","        spent_sun = prev[\"sun_before_action\"] - prev[\"sun_after_action\"]\n","\n","        reward = (\n","            earned_sun - spent_sun * (1.001 ** prev[\"sun_spent_before_action\"])\n","        ) / 25\n","        if game_status == GameStatus.WIN:\n","            reward += self.get_sun() / 25\n","        return reward\n","\n","    def _get_reward_plain(self, prev, game_status):\n","        # if game_status == GameStatus.LOSE:\n","        #     return 0\n","        return (self.get_sun() - prev[\"sun_before_action\"]) / 25\n","\n","    def _get_reward(self, prev, action, game_status):\n","        # return self._get_reward_plain(prev, game_status)\n","\n","        # if game_status == GameStatus.LOSE:\n","        #     return -72\n","\n","        reward = (self.get_sun() - prev[\"sun_before_action\"]) / 25\n","        # if game_status == GameStatus.WIN:\n","        #     reward += self.get_sun() / 25 / 2\n","        # if game_status == GameStatus.LOSE:\n","        #     reward -= 5\n","        return reward\n","\n","    def _reset_world(self) -> None:\n","        self.world = World(SceneType.night)\n","        self.world.scene.stop_spawn = True\n","        self.world.scene.is_iz = True\n","        self.world.scene.set_sun(150)\n","        plant_list = [\n","            plant for plant, count in plant_counts.items() for _ in range(count)\n","        ]\n","        np.random.shuffle(plant_list)\n","        for index, plant in enumerate(plant_list):\n","            self.world.plant_factory.create(\n","                plant,\n","                index // P_LANE_LENGTH,\n","                index % P_LANE_LENGTH,\n","            )\n","        self._update_state()\n","\n","    def _take_action(self, action):\n","        if action > 0:\n","            action -= 1\n","            z_idx = action // N_LANES\n","            row = action % N_LANES\n","            col = 4\n","            sun = self.get_sun() - zombie_deck[z_idx][1]\n","            self.sun_spent += zombie_deck[z_idx][1]\n","            assert sun >= 0\n","            self.world.zombie_factory.create(zombie_deck[z_idx][0], row, col)\n","            self.world.scene.set_sun(sun)\n","\n","    def _update_state(self):\n","        self.state, self.zombie_count, self.plant_count = self.ob_factory.create(\n","            self.world\n","        )\n","        self.state.append(self.sun_spent / 1950)\n","\n","        self.brains = []\n","        for i, b in enumerate(self.state[BRAIN_BASE : BRAIN_BASE + 5]):\n","            if b > 0.5:\n","                self.brains.append(i)\n","\n","    def print_human_readable_state(self, highlight=None):\n","        def plant_str(plant_type):\n","            if plant_type == 1:\n","                return \"sun\"\n","            if plant_type == 2:\n","                return \"pea\"\n","            if plant_type == 3:\n","                return \"sqa\"\n","            if plant_type == 4:\n","                return \"sno\"\n","            return \"---\"\n","\n","        def zombie_x_to_col(x):\n","            col = int((x + 40) // 80)\n","            return min(max(col, 0), LANE_LENGTH - 1)\n","\n","        def zombie_str(zombie_type):\n","            if zombie_type == 1:\n","                return \"Z\"\n","            if zombie_type == 2:\n","                return \"B\"\n","            if zombie_type == 3:\n","                return \"F\"\n","            return \".\"\n","\n","        def acc1_hp_max(zombie_type):\n","            if zombie_type == 2:\n","                return 1100\n","            if zombie_type == 3:\n","                return 1400\n","            return 0\n","\n","        plant_hps = [[0 for _ in range(P_LANE_LENGTH)] for _ in range(N_LANES)]\n","        plant_types = [[\"---\" for _ in range(P_LANE_LENGTH)] for _ in range(N_LANES)]\n","        zombie_hps = [[0 for _ in range(LANE_LENGTH)] for _ in range(N_LANES)]\n","        zombie_types = [[\".\" for _ in range(LANE_LENGTH)] for _ in range(N_LANES)]\n","\n","        state = self.state\n","\n","        for i in range(NUM_PLANTS):\n","            base = NUM_ZOMBIES * ZOMBIE_SIZE + i * PLANT_SIZE\n","            if state[base] != 0:\n","                plant_type = int(round(state[base] * 4))\n","                hp = state[base + 1]\n","                row = int(round(state[base + 2] * 5))\n","                col = int(round(state[base + 3] * 9))\n","\n","                plant_hps[row][col] += hp * 300\n","                plant_types[row][col] = plant_str(plant_type)\n","\n","        for i in range(NUM_ZOMBIES):\n","            base = i * ZOMBIE_SIZE\n","            if state[base] != 0:\n","                zombie_type = int(round(state[base] * 3))\n","                x = state[base + 1] * 650\n","                row = int(round(state[base + 2] * 5))\n","                hp = state[base + 3]\n","                acc1_hp = state[base + 4]\n","                col = zombie_x_to_col(x)\n","\n","                zombie_hps[row][col] += hp * 270 + acc1_hp * acc1_hp_max(zombie_type)\n","                zombie_types[row][col] = zombie_str(zombie_type)\n","\n","        print(\"==Plant HP==\")\n","        for row in range(N_LANES):\n","            print(f\"row {row+1}: \", end=\"\")\n","            for col in range(P_LANE_LENGTH):\n","                print(f\"{plant_hps[row][col]:.2f}\\t\", end=\"\")\n","            print()\n","\n","        print(\"==Plant Type==\")\n","        for row in range(N_LANES):\n","            print(f\"row {row+1}: \", end=\"\")\n","            for col in range(P_LANE_LENGTH):\n","                print(f\"{plant_types[row][col]}\\t\", end=\"\")\n","            print()\n","\n","        print(\"==Zombie HP==\")\n","        for row in range(N_LANES):\n","            print(f\"row {row+1}: \", end=\"\")\n","            for col in range(LANE_LENGTH):\n","                print(f\"{zombie_hps[row][col]:.2f}\\t\", end=\"\")\n","            print()\n","\n","        print(\"==Zombie Type==\")\n","        highlight_row, highlight_col = (-1, -1) if highlight is None else highlight\n","        for row in range(N_LANES):\n","            print(f\"row {row+1}: \", end=\"\")\n","            for col in range(LANE_LENGTH):\n","                out = f\"{zombie_types[row][col]}\"\n","                if row == highlight_row and col == highlight_col:\n","                    out = f\"[{out}]\"\n","                out += \"\\t\"\n","                print(out, end=\"\")\n","            print()\n","\n","        print(\n","            f\"Step: {self.step_count}; Sun: {self.get_sun()}; Brains: {len(self.brains)}; Game status: {self._get_game_status().name} \"\n","        )"],"metadata":{"id":"aAwHz-3D4SGV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"ZwuPV_EGVr1U"}},{"cell_type":"markdown","source":["Epsilon"],"metadata":{"id":"q3Mk7Kq1V3v-"}},{"cell_type":"code","source":["import warnings\n","\n","\n","class Epsilons:\n","    def __init__(\n","        self,\n","        seq_length,\n","        start_epsilon,\n","        end_epsilon,\n","        interpolation=\"exponential\",\n","    ):\n","        self.seq_length = seq_length\n","        self.start_epsilon = start_epsilon\n","        self.end_epsilon = end_epsilon\n","        self.interpolation = interpolation\n","\n","        assert seq_length > 1, \"seq_length must be positive\"\n","        assert interpolation in [\"exponential\"], \"not implemented\"\n","\n","        self.index = 0\n","        self.decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n","            1 / (self.seq_length - 1)\n","        )\n","        self.has_warned = False\n","\n","    def get(self):\n","        return self.start_epsilon * self.decay_rate**self.index\n","\n","    def next(self):\n","        self.index += 1\n","        if self.index >= self.seq_length:\n","            if not self.has_warned:\n","                self.has_warned = True\n","                warnings.warn(\n","                    f\"index = {self.index} overflows for seq_length = {self.seq_length}, using index = {self.seq_length - 1}.\"\n","                )\n","            self.index = self.seq_length - 1"],"metadata":{"id":"gGV02Lfx4TqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluate Agent"],"metadata":{"id":"LufQ8NlaXHsg"}},{"cell_type":"code","source":["from collections import Counter\n","import numpy as np\n","import os\n","\n","\n","def evaluate_agent(agent, test_size=500, step_count=None, output_file=None):\n","    agent.set_to_eval_mode()\n","    game_results = []\n","    steps = []\n","    winning_suns = []\n","\n","    for test_idx in range(1, test_size + 1):\n","        print(f\"\\rTesting {test_idx}/{test_size}...\", end=\"\")\n","        env = IZenv()\n","        state, mask = env.get_state_and_mask()\n","\n","        for step in range(1_000_000):\n","            action = agent.get_best_q_action(state, env.get_valid_actions(mask))\n","            _, next_state, next_mask, game_status = env.step(action)\n","            state, mask = next_state, next_mask\n","\n","            if game_status != GameStatus.CONTINUE:\n","                game_results.append(game_status)\n","                steps.append(step)\n","                if game_status == GameStatus.WIN:\n","                    winning_suns.append(env.get_sun())\n","                break\n","\n","    print()\n","\n","    results_counter = Counter(game_results)\n","    total = len(game_results)\n","    percentages = {}\n","    for status in GameStatus:\n","        percentages[status] = (\n","            (results_counter.get(status, 0) / total) * 100 if total > 0 else 0\n","        )\n","\n","    if output_file is None:\n","        if step_count is not None:\n","            print(f\"Step: {format_num(step_count)}\")\n","        for status in GameStatus:\n","            count = results_counter.get(status, 0)\n","            if count != 0:\n","                print(f\"{status.name}: {count}/{total} ({percentages[status]:.2f}%)\")\n","        print(f\"Mean steps: {np.mean(steps):.2f}\")\n","        if (len(winning_suns)) > 0:\n","            print(f\"Mean winning suns: {np.mean(winning_suns):.2f}\")\n","    else:\n","        create_folder_if_not_exist(os.path.dirname(output_file))\n","        start_new_file = not os.path.exists(output_file)\n","        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n","            if start_new_file:\n","                f.write(\"step,test_size,win,mean_winning_suns,mean_steps,\\n\")\n","            f.write(\n","                f\"{format_num(step_count)},{test_size},{percentages[GameStatus.WIN]:.2f}%,{np.mean(winning_suns):.2f},{np.mean(steps):.2f}\\n\"\n","            )\n","\n","    agent.set_to_training_mode()\n","\n","    winning_rate = results_counter.get(GameStatus.WIN, 0) / len(game_results)\n","    mean_winning_sun = np.mean(winning_suns) if len(winning_suns) > 0 else -1\n","    return winning_rate, mean_winning_sun\n","\n","\n","def manually_test_agent(agent, fix_rand=True):\n","    agent.set_to_eval_mode()\n","\n","    if fix_rand:\n","        np.random.seed(0)\n","    else:\n","        np.random.seed()\n","\n","    env = IZenv()\n","    state, mask = env.get_state_and_mask()\n","    last_step = 0\n","\n","    for step in range(10000):\n","        action = agent.get_best_q_action(state, env.get_valid_actions(mask))\n","        reward, next_state, next_mask, game_status = env.step(action)\n","\n","        if action != 0 or game_status != GameStatus.CONTINUE:\n","            env.print_human_readable_state(\n","                highlight=((action - 1) % 5, 4) if action != 0 else None\n","            )\n","            print(f\"Action: {action}, Reward: {reward}, ΔStep: {step - last_step}\")\n","            last_step = step\n","            _ = input(\"\")\n","\n","        state, mask = next_state, next_mask\n","\n","        if game_status != GameStatus.CONTINUE:\n","            break\n","\n","    agent.set_to_training_mode()"],"metadata":{"id":"w7Uuf0lD4Uod"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Noisy Layer"],"metadata":{"id":"p1b2cJvFXSyy"}},{"cell_type":"code","source":["import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class NoisyLinear(nn.Module):\n","    \"\"\"Noisy linear module for NoisyNet.\n","\n","\n","\n","    Attributes:\n","        in_features (int): input size of linear module\n","        out_features (int): output size of linear module\n","        std_init (float): initial std value\n","        weight_mu (nn.Parameter): mean value weight parameter\n","        weight_sigma (nn.Parameter): std value weight parameter\n","        bias_mu (nn.Parameter): mean value bias parameter\n","        bias_sigma (nn.Parameter): std value bias parameter\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_features: int,\n","        out_features: int,\n","        std_init: float = 0.5,\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        super(NoisyLinear, self).__init__()\n","\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.std_init = std_init\n","\n","        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.register_buffer(\"weight_epsilon\", torch.Tensor(out_features, in_features))\n","\n","        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n","        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n","        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n","\n","        self.reset_parameters()\n","        self.reset_noise()\n","\n","    def reset_parameters(self):\n","        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n","        mu_range = 1 / math.sqrt(self.in_features)\n","        self.weight_mu.data.uniform_(-mu_range, mu_range)\n","        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n","        self.bias_mu.data.uniform_(-mu_range, mu_range)\n","        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n","\n","    def reset_noise(self):\n","        \"\"\"Make new noise.\"\"\"\n","        epsilon_in = self.scale_noise(self.in_features)\n","        epsilon_out = self.scale_noise(self.out_features)\n","\n","        # outer product\n","        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n","        self.bias_epsilon.copy_(epsilon_out)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\n","\n","        We don't use separate statements on train / eval mode.\n","        It doesn't show remarkable difference of performance.\n","        \"\"\"\n","        return F.linear(\n","            x,\n","            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n","            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n","        )\n","\n","    @staticmethod\n","    def scale_noise(size: int) -> torch.Tensor:\n","        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n","        x = torch.randn(size)\n","\n","        return x.sign().mul(x.abs().sqrt())"],"metadata":{"id":"cd0wnuOBXSYt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Util"],"metadata":{"id":"yRsEA7Y7XkiN"}},{"cell_type":"code","source":["import os\n","import datetime\n","\n","\n","def get_timestamp():\n","    return datetime.datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")\n","\n","\n","def create_folder_if_not_exist(folder_name):\n","    current_directory = os.getcwd()\n","    folder_path = os.path.join(current_directory, folder_name)\n","    if not os.path.exists(folder_path):\n","        os.makedirs(folder_path)\n","\n","\n","def format_num(n):\n","    if n > 1_000_000:\n","        if n % 1_000_000 == 0:\n","            return f\"{int(n / 1_000_000)}m\"\n","        return f\"{n / 1_000_000}m\"\n","    if n > 1_000:\n","        if n % 1_000 == 0:\n","            return f\"{int(n / 1_000)}k\"\n","        return f\"{n / 1_000}k\"\n","    return str(n)"],"metadata":{"id":"_O92wAQaXnSz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["threshold"],"metadata":{"id":"8fbiTTYKXqUT"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Feb 23 15:25:15 2021\n","\n","@author: Lukas Frank\n","\"\"\"\n","import math\n","import numpy as np\n","import warnings\n","\n","\n","class Threshold:\n","    \"\"\"\n","    Generate sequences of epsilon thresholds.\n","\n","    :param seq_length: int, length of epsilon sequence = number of epsilons to draw\n","\n","    :param start_epsilon: float, value to start with\n","\n","    :param end_epsilon (optional): float, value to end with. If None, return constant\n","        sequence of value start_epsilon. Default: None.\n","\n","    :param interpolation (optional): string, interpolation method:\\n\n","        either 'linear', 'exponential' or 'sinusoidal'. Default: 'linear'.\n","        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf\n","\n","    :param periods: int, number of periods for sinusoidal sequence. Default: 10.\n","        ...\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        seq_length,\n","        start_epsilon,\n","        end_epsilon=None,\n","        interpolation=\"linear\",\n","        periods=10,\n","    ):\n","        self.seq_length = seq_length\n","        self.start_epsilon = start_epsilon\n","        assert interpolation in [\n","            \"linear\",\n","            \"exponential\",\n","            \"sinusoidal\",\n","        ], \"interpolation argument invalid. Must be 'linear', 'exponential', 'sinusoidal' or unspecified.\"\n","        self.interpolation = interpolation\n","        if end_epsilon is None:\n","            self.end_epsilon = start_epsilon\n","            # set to linear to deliver constant sequence of epsilons.\n","            self.interpolation = \"linear\"\n","        else:\n","            self.end_epsilon = end_epsilon\n","        self.periods = periods\n","\n","    def epsilon(self, index=None):\n","        \"\"\"Return sequence or element of sequence of epsilons as specified\n","\n","        :param index (optional): index of sequence element to be returned. If None, return\n","            full sequence. Default: None.\\n\n","\n","        :return: array-like with shape (self.seq_length) or a single float value.\n","        \"\"\"\n","        epsilon = None\n","\n","        if self.interpolation == \"linear\":\n","            epsilon = self._linear(index)\n","\n","        elif self.interpolation == \"exponential\":\n","            epsilon = self._exponential(index)\n","\n","        elif self.interpolation == \"sinusoidal\":\n","            epsilon = self._sinusoidal(index, self.periods)\n","\n","        return epsilon\n","\n","    def _linear(self, index):\n","        \"\"\"Calls linear calculation method depending on whether index is given or not.\"\"\"\n","        if index is not None:  # return only one epsilon\n","            self._check_index_length(index)\n","            return self._linear_point(index)\n","        else:\n","            return self._linear_sequence()\n","\n","    def _exponential(self, index):\n","        \"\"\"Calls exponential calculation depending on whether index is given or not.\"\"\"\n","        if index is not None:  # return only one epsilon\n","            self._check_index_length(index)\n","            return self._exponential_point(index)\n","        else:\n","            return self._exponential_sequence()\n","\n","    def _sinusoidal(self, index, periods):\n","        \"\"\"Calls sinusoidal calculation depending on whether index is given or not.\"\"\"\n","        if index is not None:  # return only one epsilon\n","            self._check_index_length(index)\n","            return self._sinusoidal_point(index, mini_epochs=periods)\n","        else:\n","            return self._sinusoidal_sequence(mini_epochs=periods)\n","\n","    def _linear_sequence(self):\n","        \"\"\"Computes linear sequence\"\"\"\n","        return np.linspace(\n","            start=self.start_epsilon, stop=self.end_epsilon, num=self.seq_length\n","        ).tolist()\n","\n","    def _linear_point(self, index):\n","        \"\"\"Computes a single point by linear interpolation\"\"\"\n","        return (\n","            self.start_epsilon\n","            + (self.end_epsilon - self.start_epsilon) / (self.seq_length - 1) * index\n","        )\n","\n","    def _exponential_sequence(self):\n","        \"\"\"Computes exponential sequence\"\"\"\n","        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n","            1 / (self.seq_length - 1)\n","        )\n","        return [(self.start_epsilon * decay_rate**i) for i in range(self.seq_length)]\n","\n","    def _exponential_point(self, index):\n","        \"\"\"Computes a single point by exponential interpolation\"\"\"\n","        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n","            1 / (self.seq_length - 1)\n","        )\n","        return self.start_epsilon * decay_rate**index\n","\n","    def _sinusoidal_sequence(self, mini_epochs):\n","        \"\"\"Computes sinusoidal sequence.\n","\n","        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf \\n\n","\n","        :param mini_epochs (optional): int, number of oscillations in sequence.\n","        \"\"\"\n","        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n","            1 / (self.seq_length - 1)\n","        )\n","        return [\n","            (\n","                self.start_epsilon\n","                * decay_rate**i\n","                * 0.5\n","                * (1 + np.cos(2 * math.pi * i * mini_epochs / (self.seq_length - 1)))\n","            )\n","            for i in range(self.seq_length)\n","        ]\n","\n","    def _sinusoidal_point(self, index, mini_epochs):\n","        \"\"\"Computes a single point by sinusoidal interpolation.\n","\n","        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf \\n\n","\n","        :param mini_epochs (optional): int, number of oscillations in sequence.\n","        \"\"\"\n","        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n","            1 / (self.seq_length - 1)\n","        )\n","        return (\n","            self.start_epsilon\n","            * decay_rate**index\n","            * 0.5\n","            * (1 + np.cos(2 * math.pi * index * mini_epochs / (self.seq_length - 1)))\n","        )\n","\n","    def _check_index_length(self, index):\n","        \"\"\"Check whether index is in sequence.\"\"\"\n","        if index >= self.seq_length:\n","            warnings.warn(\n","                f\"threshold.epsilon(index): index {index} > seq_length {self.seq_length}.\"\n","                f\"Changing index to {index - self.seq_length}.\"\n","            )"],"metadata":{"id":"5Gk1qbGFXtxz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Segment Tree"],"metadata":{"id":"1RHmcNTjX5er"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"Segment tree for Prioritized Replay Buffer.\"\"\"\n","\n","import operator\n","from typing import Callable\n","\n","\n","class SegmentTree:\n","    \"\"\" Create SegmentTree.\n","\n","    Taken from OpenAI baselines github repository:\n","    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n","\n","    Attributes:\n","        capacity (int)\n","        tree (list)\n","        operation (function)\n","\n","    \"\"\"\n","\n","    def __init__(self, capacity: int, operation: Callable, init_value: float):\n","        \"\"\"Initialization.\n","\n","        Args:\n","            capacity (int)\n","            operation (function)\n","            init_value (float)\n","\n","        \"\"\"\n","        assert (\n","            capacity > 0 and capacity & (capacity - 1) == 0\n","        ), \"capacity must be positive and a power of 2.\"\n","        self.capacity = capacity\n","        self.tree = [init_value for _ in range(2 * capacity)]\n","        self.operation = operation\n","\n","    def _operate_helper(\n","        self, start: int, end: int, node: int, node_start: int, node_end: int\n","    ) -> float:\n","        \"\"\"Returns result of operation in segment.\"\"\"\n","        if start == node_start and end == node_end:\n","            return self.tree[node]\n","        mid = (node_start + node_end) // 2\n","        if end <= mid:\n","            return self._operate_helper(start, end, 2 * node, node_start, mid)\n","        else:\n","            if mid + 1 <= start:\n","                return self._operate_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","            else:\n","                return self.operation(\n","                    self._operate_helper(start, mid, 2 * node, node_start, mid),\n","                    self._operate_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),\n","                )\n","\n","    def operate(self, start: int = 0, end: int = 0) -> float:\n","        \"\"\"Returns result of applying `self.operation`.\"\"\"\n","        if end <= 0:\n","            end += self.capacity\n","        end -= 1\n","\n","        return self._operate_helper(start, end, 1, 0, self.capacity - 1)\n","\n","    def __setitem__(self, idx: int, val: float):\n","        \"\"\"Set value in tree.\"\"\"\n","        idx += self.capacity\n","        self.tree[idx] = val\n","\n","        idx //= 2\n","        while idx >= 1:\n","            self.tree[idx] = self.operation(self.tree[2 * idx], self.tree[2 * idx + 1])\n","            idx //= 2\n","\n","    def __getitem__(self, idx: int) -> float:\n","        \"\"\"Get real value in leaf node of tree.\"\"\"\n","        assert 0 <= idx < self.capacity\n","\n","        return self.tree[self.capacity + idx]\n","\n","\n","class SumSegmentTree(SegmentTree):\n","    \"\"\" Create SumSegmentTree.\n","\n","    Taken from OpenAI baselines github repository:\n","    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n","\n","    \"\"\"\n","\n","    def __init__(self, capacity: int):\n","        \"\"\"Initialization.\n","\n","        Args:\n","            capacity (int)\n","\n","        \"\"\"\n","        super(SumSegmentTree, self).__init__(\n","            capacity=capacity, operation=operator.add, init_value=0.0\n","        )\n","\n","    def sum(self, start: int = 0, end: int = 0) -> float:\n","        \"\"\"Returns arr[start] + ... + arr[end].\"\"\"\n","        return super(SumSegmentTree, self).operate(start, end)\n","\n","    def retrieve(self, upperbound: float) -> int:\n","        \"\"\"Find the highest index `i` about upper bound in the tree\"\"\"\n","        # TODO: Check assert case and fix bug\n","        assert 0 <= upperbound <= self.sum() + 1e-5, \"upperbound: {}\".format(upperbound)\n","\n","        idx = 1\n","\n","        while idx < self.capacity:  # while non-leaf\n","            left = 2 * idx\n","            right = left + 1\n","            if self.tree[left] > upperbound:\n","                idx = 2 * idx\n","            else:\n","                upperbound -= self.tree[left]\n","                idx = right\n","        return idx - self.capacity\n","\n","\n","class MinSegmentTree(SegmentTree):\n","    \"\"\" Create SegmentTree.\n","\n","    Taken from OpenAI baselines github repository:\n","    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n","\n","    \"\"\"\n","\n","    def __init__(self, capacity: int):\n","        \"\"\"Initialization.\n","\n","        Args:\n","            capacity (int)\n","\n","        \"\"\"\n","        super(MinSegmentTree, self).__init__(\n","            capacity=capacity, operation=min, init_value=float(\"inf\")\n","        )\n","\n","    def min(self, start: int = 0, end: int = 0) -> float:\n","        \"\"\"Returns min(arr[start], ...,  arr[end]).\"\"\"\n","        return super(MinSegmentTree, self).operate(start, end)"],"metadata":{"id":"F4d14OvUX7nk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Replay Buffer"],"metadata":{"id":"K-SJbQ7VX_jh"}},{"cell_type":"code","source":["from collections import deque\n","from typing import Tuple, Dict, Deque, List\n","import numpy as np\n","import random\n","\n","\n","class ReplayBuffer:\n","    \"\"\"A simple numpy replay buffer.\"\"\"\n","\n","    def __init__(\n","        self,\n","        obs_dim: int,\n","        size: int,\n","        batch_size: int = 32,\n","        n_step: int = 1,\n","        gamma: float = 0.99,\n","    ):\n","        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n","        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n","        self.acts_buf = np.zeros([size], dtype=np.float32)\n","        self.rews_buf = np.zeros([size], dtype=np.float32)\n","        self.done_buf = np.zeros(size, dtype=np.float32)\n","        self.max_size, self.batch_size = size, batch_size\n","        (\n","            self.ptr,\n","            self.size,\n","        ) = (\n","            0,\n","            0,\n","        )\n","\n","        # for N-step Learning\n","        self.n_step_buffer = deque(maxlen=n_step)\n","        self.n_step = n_step\n","        self.gamma = gamma\n","\n","    def store(\n","        self,\n","        obs: np.ndarray,\n","        act: np.ndarray,\n","        rew: float,\n","        next_obs: np.ndarray,\n","        done: bool,\n","    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n","        transition = (obs, act, rew, next_obs, done)\n","        self.n_step_buffer.append(transition)\n","\n","        # single step transition is not ready\n","        if len(self.n_step_buffer) < self.n_step:\n","            return ()\n","\n","        # make a n-step transition\n","        rew, next_obs, done = self._get_n_step_info(self.n_step_buffer, self.gamma)\n","        obs, act = self.n_step_buffer[0][:2]\n","\n","        self.obs_buf[self.ptr] = obs\n","        self.next_obs_buf[self.ptr] = next_obs\n","        self.acts_buf[self.ptr] = act\n","        self.rews_buf[self.ptr] = rew\n","        self.done_buf[self.ptr] = done\n","        self.ptr = (self.ptr + 1) % self.max_size\n","        self.size = min(self.size + 1, self.max_size)\n","\n","        return self.n_step_buffer[0]\n","\n","    def sample_batch(self) -> Dict[str, np.ndarray]:\n","        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n","\n","        return dict(\n","            obs=self.obs_buf[idxs],\n","            next_obs=self.next_obs_buf[idxs],\n","            acts=self.acts_buf[idxs],\n","            rews=self.rews_buf[idxs],\n","            done=self.done_buf[idxs],\n","            # for N-step Learning\n","            indices=idxs,\n","        )\n","\n","    def sample_batch_from_idxs(self, idxs: np.ndarray) -> Dict[str, np.ndarray]:\n","        # for N-step Learning\n","        return dict(\n","            obs=self.obs_buf[idxs],\n","            next_obs=self.next_obs_buf[idxs],\n","            acts=self.acts_buf[idxs],\n","            rews=self.rews_buf[idxs],\n","            done=self.done_buf[idxs],\n","        )\n","\n","    def _get_n_step_info(\n","        self, n_step_buffer: Deque, gamma: float\n","    ) -> Tuple[np.int64, np.ndarray, bool]:\n","        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n","        # info of the last transition\n","        rew, next_obs, done = n_step_buffer[-1][-3:]\n","\n","        for transition in reversed(list(n_step_buffer)[:-1]):\n","            r, n_o, d = transition[-3:]\n","\n","            rew = r + gamma * rew * (1 - d)\n","            next_obs, done = (n_o, d) if d else (next_obs, done)\n","\n","        return rew, next_obs, done\n","\n","    def __len__(self) -> int:\n","        return self.size\n","\n","\n","class PrioritizedReplayBuffer(ReplayBuffer):\n","    \"\"\"Prioritized Replay buffer.\n","\n","    Attributes:\n","        max_priority (float): max priority\n","        tree_ptr (int): next index of tree\n","        alpha (float): alpha parameter for prioritized replay buffer\n","        sum_tree (SumSegmentTree): sum tree for prior\n","        min_tree (MinSegmentTree): min tree for min prior to get max weight\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        obs_dim: int,\n","        size: int,\n","        batch_size: int = 32,\n","        alpha: float = 0.6,\n","        n_step: int = 1,\n","        gamma: float = 0.99,\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        assert alpha >= 0\n","\n","        super(PrioritizedReplayBuffer, self).__init__(\n","            obs_dim, size, batch_size, n_step, gamma\n","        )\n","        self.max_priority, self.tree_ptr = 1.0, 0\n","        self.alpha = alpha\n","\n","        # capacity must be positive and a power of 2.\n","        tree_capacity = 1\n","        while tree_capacity < self.max_size:\n","            tree_capacity *= 2\n","\n","        self.sum_tree = SumSegmentTree(tree_capacity)\n","        self.min_tree = MinSegmentTree(tree_capacity)\n","\n","    def store(\n","        self,\n","        obs: np.ndarray,\n","        act: int,\n","        rew: float,\n","        next_obs: np.ndarray,\n","        done: bool,\n","    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n","        \"\"\"Store experience and priority.\"\"\"\n","        transition = super().store(obs, act, rew, next_obs, done)\n","\n","        if transition:\n","            self.sum_tree[self.tree_ptr] = self.max_priority**self.alpha\n","            self.min_tree[self.tree_ptr] = self.max_priority**self.alpha\n","            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n","\n","        return transition\n","\n","    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n","        \"\"\"Sample a batch of experiences.\"\"\"\n","        assert len(self) >= self.batch_size\n","        assert beta > 0\n","\n","        indices = self._sample_proportional()\n","\n","        obs = self.obs_buf[indices]\n","        next_obs = self.next_obs_buf[indices]\n","        acts = self.acts_buf[indices]\n","        rews = self.rews_buf[indices]\n","        done = self.done_buf[indices]\n","        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n","\n","        return dict(\n","            obs=obs,\n","            next_obs=next_obs,\n","            acts=acts,\n","            rews=rews,\n","            done=done,\n","            weights=weights,\n","            indices=indices,\n","        )\n","\n","    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n","        \"\"\"Update priorities of sampled transitions.\"\"\"\n","        assert len(indices) == len(priorities)\n","\n","        for idx, priority in zip(indices, priorities):\n","            assert priority > 0\n","            assert 0 <= idx < len(self)\n","\n","            self.sum_tree[idx] = priority**self.alpha\n","            self.min_tree[idx] = priority**self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def _sample_proportional(self) -> List[int]:\n","        \"\"\"Sample indices based on proportions.\"\"\"\n","        indices = []\n","        p_total = self.sum_tree.sum(0, len(self) - 1)\n","        segment = p_total / self.batch_size\n","\n","        for i in range(self.batch_size):\n","            a = segment * i\n","            b = segment * (i + 1)\n","            upperbound = random.uniform(a, b)\n","            idx = self.sum_tree.retrieve(upperbound)\n","            indices.append(idx)\n","\n","        return indices\n","\n","    def _calculate_weight(self, idx: int, beta: float):\n","        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n","        # get max weight\n","        p_min = self.min_tree.min() / self.sum_tree.sum()\n","        max_weight = (p_min * len(self)) ** (-beta)\n","\n","        # calculate weights\n","        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n","        weight = (p_sample * len(self)) ** (-beta)\n","        weight = weight / max_weight\n","\n","        return weight"],"metadata":{"id":"CC4-hF7bYB8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn.utils import clip_grad_norm_\n","import numpy as np\n","from typing import Dict, Tuple\n","import datetime\n","import os\n","\n","class Network(nn.Module):\n","    def __init__(\n","        self, in_dim: int, out_dim: int, atom_size: int, support: torch.Tensor\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        super(Network, self).__init__()\n","\n","        self.support = support\n","        self.out_dim = out_dim\n","        self.atom_size = atom_size\n","\n","        # set common feature layer\n","        self.feature_layer = nn.Sequential(\n","            nn.Linear(in_dim, 128),\n","            nn.ReLU(),\n","        )\n","\n","        # set advantage layer\n","        self.advantage_hidden_layer = NoisyLinear(128, 128)\n","        self.advantage_layer = NoisyLinear(128, out_dim * atom_size)\n","\n","        # set value layer\n","        self.value_hidden_layer = NoisyLinear(128, 128)\n","        self.value_layer = NoisyLinear(128, atom_size)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\"\"\"\n","        dist = self.dist(x)\n","        q = torch.sum(dist * self.support, dim=2)\n","\n","        return q\n","\n","    def dist(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Get distribution for atoms.\"\"\"\n","        feature = self.feature_layer(x)\n","        adv_hid = F.relu(self.advantage_hidden_layer(feature))\n","        val_hid = F.relu(self.value_hidden_layer(feature))\n","\n","        advantage = self.advantage_layer(adv_hid).view(-1, self.out_dim, self.atom_size)\n","        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n","        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n","\n","        dist = F.softmax(q_atoms, dim=-1)\n","        dist = dist.clamp(min=1e-3)  # for avoiding nans\n","\n","        return dist\n","\n","    def reset_noise(self):\n","        \"\"\"Reset all noisy layers.\"\"\"\n","        self.advantage_hidden_layer.reset_noise()\n","        self.advantage_layer.reset_noise()\n","        self.value_hidden_layer.reset_noise()\n","        self.value_layer.reset_noise()\n","\n","\n","class DQNAgent:\n","    \"\"\"DQN Agent interacting with environment.\n","\n","    Attribute:\n","        env (IZenv): izombie env\n","        memory (PrioritizedReplayBuffer): replay memory to store transitions\n","        batch_size (int): batch size for sampling\n","        gamma (float): discount factor\n","        dqn (Network): model to train and select actions\n","        dqn_target (Network): target model to update\n","        optimizer (torch.optim): optimizer for training dqn\n","        transition (list): transition information including\n","                           state, action, reward, next_state, done\n","        v_min (float): min value of support\n","        v_max (float): max value of support\n","        atom_size (int): the unit number of support\n","        support (torch.Tensor): support for categorical dqn\n","        use_n_step (bool): whether to use n_step memory\n","        n_step (int): step number to calculate n-step td error\n","        memory_n (ReplayBuffer): n-step replay buffer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        env: IZenv,\n","        model_name: str,\n","        device: str,\n","        memory_size: int,\n","        batch_size: int,\n","        gamma: float = 0.99,\n","        lr: float = 1e-3,\n","        # PER parameters\n","        alpha: float = 0.2,\n","        beta: float = 0.6,\n","        prior_eps: float = 1e-6,\n","        # Categorical DQN parameters\n","        v_min: float = 0.0,\n","        v_max: float = 200.0,\n","        atom_size: int = 51,\n","        # N-step Learning\n","        n_step: int = 3,\n","    ):\n","        \"\"\"Initialization.\n","\n","        Args:\n","            env (gym.Env): openAI Gym environment\n","            memory_size (int): length of memory\n","            batch_size (int): batch size for sampling\n","            lr (float): learning rate\n","            gamma (float): discount factor\n","            alpha (float): determines how much prioritization is used\n","            beta (float): determines how much importance sampling is used\n","            prior_eps (float): guarantees every transition can be sampled\n","            v_min (float): min value of support\n","            v_max (float): max value of support\n","            atom_size (int): the unit number of support\n","            n_step (int): step number to calculate n-step td error\n","        \"\"\"\n","        if atom_size is None:\n","            atom_size = v_max - v_min + 1\n","        obs_dim = STATE_SIZE\n","        action_dim = ACTION_SIZE\n","\n","        self.env = env\n","        self.model_name = model_name\n","        self.batch_size = batch_size\n","        self.gamma = gamma\n","        # NoisyNet: All attributes related to epsilon are removed\n","\n","        # device: cpu / gpu\n","        self.device = torch.device(device)\n","        print(f\"Using {self.device} device.\")\n","\n","        # PER\n","        # memory for 1-step Learning\n","        self.beta = beta\n","        self.prior_eps = prior_eps\n","        self.memory = PrioritizedReplayBuffer(\n","            obs_dim, memory_size, batch_size, alpha=alpha, gamma=gamma\n","        )\n","\n","        # memory for N-step Learning\n","        self.use_n_step = True if n_step > 1 else False\n","        if self.use_n_step:\n","            self.n_step = n_step\n","            self.memory_n = ReplayBuffer(\n","                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n","            )\n","\n","        # Categorical DQN parameters\n","        self.v_min = v_min\n","        self.v_max = v_max\n","        self.atom_size = atom_size\n","        self.support = torch.linspace(self.v_min, self.v_max, self.atom_size).to(\n","            self.device\n","        )\n","\n","        # networks: dqn, dqn_target\n","        self.dqn = Network(obs_dim, action_dim, self.atom_size, self.support).to(\n","            self.device\n","        )\n","        self.dqn_target = Network(obs_dim, action_dim, self.atom_size, self.support).to(\n","            self.device\n","        )\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","        self.dqn_target.eval()\n","\n","        # optimizer\n","        self.optimizer = optim.Adam(self.dqn.parameters(), lr=lr)\n","\n","        # transition to store in memory\n","        self.transition = list()\n","\n","        # mode: train / test\n","        self.is_test = False\n","\n","        # stats\n","        self.winning_suns = []\n","        self.losses = []\n","        self.game_results = []\n","        self.steps = []\n","        self.scores = []\n","\n","    def get_best_q_action(self, state, mask):\n","        \"\"\"Select an action from the input state.\"\"\"\n","        # NoisyNet: no epsilon greedy action selection\n","        with torch.no_grad():\n","            valid_actions = self.env.get_valid_actions(mask)\n","            q_values = self.dqn(\n","                torch.FloatTensor(state).unsqueeze(0).to(self.device)\n","            ).detach()\n","            valid_q_values = q_values[0, valid_actions]\n","            max_q_index = torch.argmax(valid_q_values).item()\n","            selected_action = valid_actions[max_q_index]\n","\n","            if not self.is_test:\n","                self.transition = [state, selected_action]\n","\n","            return selected_action\n","\n","    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n","        \"\"\"Take an action and return the response of the env.\"\"\"\n","        reward, next_state, next_mask, game_status = self.env.step(action)\n","        done = game_status != GameStatus.CONTINUE\n","\n","        if not self.is_test:\n","            self.transition += [reward, next_state, done]\n","\n","            # N-step transition\n","            if self.use_n_step:\n","                one_step_transition = self.memory_n.store(*self.transition)\n","            # 1-step transition\n","            else:\n","                one_step_transition = self.transition\n","\n","            # add a single step transition\n","            if one_step_transition:\n","                self.memory.store(*one_step_transition)\n","\n","        return next_state, next_mask, reward, game_status, done\n","\n","    def update_main_model(self) -> torch.Tensor:\n","        \"\"\"Update the model by gradient descent.\"\"\"\n","        # PER needs beta to calculate weights\n","        samples = self.memory.sample_batch(self.beta)\n","        weights = torch.FloatTensor(samples[\"weights\"].reshape(-1, 1)).to(self.device)\n","        indices = samples[\"indices\"]\n","\n","        # 1-step Learning loss\n","        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n","\n","        # PER: importance sampling before average\n","        loss = torch.mean(elementwise_loss * weights)\n","\n","        # N-step Learning loss\n","        # we are gonna combine 1-step loss and n-step loss so as to\n","        # prevent high-variance. The original rainbow employs n-step loss only.\n","        if self.use_n_step:\n","            gamma = self.gamma**self.n_step\n","            samples = self.memory_n.sample_batch_from_idxs(indices)\n","            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n","            elementwise_loss += elementwise_loss_n_loss\n","\n","            # PER: importance sampling before average\n","            loss = torch.mean(elementwise_loss * weights)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(self.dqn.parameters(), 10.0)\n","        self.optimizer.step()\n","\n","        # PER: update priorities\n","        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n","        new_priorities = loss_for_prior + self.prior_eps\n","        self.memory.update_priorities(indices, new_priorities)\n","\n","        # NoisyNet: reset noise\n","        self.dqn.reset_noise()\n","        self.dqn_target.reset_noise()\n","\n","        return loss.item()\n","\n","    def train(\n","        self,\n","        num_steps: int,\n","        stats_window: int = 1_000,\n","        print_stats_every=30_000,\n","        update_target_every=2000,\n","        update_main_every=1,\n","        save_every=None,\n","        eval_every=None,\n","    ):\n","        \"\"\"Train the agent.\"\"\"\n","        model_dir = f\"model/{self.model_name}_{get_timestamp()}\"\n","        self.is_test = False\n","        self.set_to_training_mode()\n","\n","        state, mask = self.env.reset()\n","        start_time = datetime.datetime.now()\n","        score = 0\n","\n","        for step_idx in range(1, num_steps + 1):\n","            action = self.get_best_q_action(state, mask)\n","            state, mask, reward, game_status, done = self.step(action)\n","            score += reward\n","\n","            # NoisyNet: removed decrease of epsilon\n","\n","            # PER: increase beta\n","            fraction = min(step_idx / num_steps, 1.0)\n","            self.beta = self.beta + fraction * (1.0 - self.beta)\n","\n","            # if episode ends\n","            if done:\n","                self.game_results.append(game_status)\n","                self.steps.append(self.env.step_count)\n","                self.scores.append(score)\n","                score = 0\n","                if game_status == GameStatus.WIN:\n","                    self.winning_suns.append(self.env.get_sun())\n","                state, mask = self.env.reset()\n","\n","            # if training is ready\n","            if (\n","                len(self.memory) >= self.batch_size\n","                and step_idx % update_main_every == 0\n","            ):\n","                loss = self.update_main_model()\n","                self.losses.append(loss)\n","\n","            # if hard update is needed\n","            if step_idx % update_target_every == 0:\n","                self._sync_target_with_main()\n","\n","            if step_idx % print_stats_every == 0:\n","                self.print_stats(stats_window, step_idx, num_steps, start_time)\n","\n","            if eval_every is not None and step_idx % eval_every == 0:\n","                evaluate_agent(\n","                    self, step_count=step_idx, output_file=f\"{model_dir}/eval.csv\"\n","                )\n","\n","            if (\n","                save_every is not None and step_idx % save_every == 0\n","            ) or step_idx == num_steps:\n","                self.save(f\"{model_dir}/{format_num(step_idx)}.pth\")\n","\n","    def _compute_dqn_loss(\n","        self, samples: Dict[str, np.ndarray], gamma: float\n","    ) -> torch.Tensor:\n","        \"\"\"Return categorical dqn loss.\"\"\"\n","        state = torch.FloatTensor(samples[\"obs\"]).to(self.device)\n","        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(self.device)\n","        action = torch.LongTensor(samples[\"acts\"]).to(self.device)\n","        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(self.device)\n","        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(self.device)\n","\n","        # Categorical DQN algorithm\n","        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n","\n","        with torch.no_grad():\n","            # Double DQN\n","            next_action = self.dqn(next_state).argmax(1)\n","            next_dist = self.dqn_target.dist(next_state)\n","            next_dist = next_dist[range(self.batch_size), next_action]\n","\n","            t_z = reward + (1 - done) * gamma * self.support\n","            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n","            b = (t_z - self.v_min) / delta_z\n","            l = b.floor().long()\n","            u = b.ceil().long()\n","\n","            offset = (\n","                torch.linspace(\n","                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n","                )\n","                .long()\n","                .unsqueeze(1)\n","                .expand(self.batch_size, self.atom_size)\n","                .to(self.device)\n","            )\n","\n","            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n","            proj_dist.view(-1).index_add_(\n","                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n","            )\n","            proj_dist.view(-1).index_add_(\n","                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n","            )\n","\n","        dist = self.dqn.dist(state)\n","        log_p = torch.log(dist[range(self.batch_size), action])\n","        elementwise_loss = -(proj_dist * log_p).sum(1)\n","\n","        return elementwise_loss\n","\n","    def _sync_target_with_main(self):\n","        \"\"\"Hard update: target <- local.\"\"\"\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","\n","    def print_stats(self, stats_window, curr_step, total_step, start_time):\n","        win_rate = (\n","            sum(1 for res in self.game_results[-stats_window:] if res == GameStatus.WIN)\n","            * 100\n","            / min(stats_window, len(self.game_results))\n","        )\n","        elasped_seconds = (datetime.datetime.now() - start_time).total_seconds()\n","        print(\n","            f\"Sp {format_num(curr_step)}/{format_num(total_step)} \"\n","            f\"Mean losses {np.mean(self.losses[-stats_window:]):.2f} \"\n","            f\"Mean winning sun {np.mean(self.winning_suns[-stats_window:]):.2f} \"\n","            f\"Mean steps {np.mean(self.steps[-stats_window:]):.2f} \"\n","            f\"Mean score {np.mean(self.scores[-stats_window:]):.2f} \"\n","            f\"Win {win_rate:.2f}% \"\n","            f\"{elasped_seconds / curr_step * 1_000_000:.2f}s/1m steps\"\n","        )\n","\n","    def set_to_training_mode(self):\n","        self.dqn.train()\n","\n","    def set_to_eval_mode(self):\n","        self.dqn.eval()\n","\n","    def save(self, filename):\n","        assert not os.path.exists(filename)\n","        create_folder_if_not_exist(os.path.dirname(filename))\n","        torch.save(self.dqn.state_dict(), filename)\n","\n","    def load(self, filename):\n","        state_dict = torch.load(filename, map_location=self.device)\n","        self.dqn.load_state_dict(state_dict)\n","        self.dqn_target.load_state_dict(state_dict)"],"metadata":{"id":"IWB1PI1UQv54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"KbXmAC1IYzb7"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import random\n","\n","\n","def set_seed(seed):\n","    def seed_torch(seed):\n","        torch.manual_seed(seed)\n","        if torch.backends.cudnn.enabled:\n","            torch.cuda.manual_seed(seed)\n","            torch.backends.cudnn.benchmark = False\n","            torch.backends.cudnn.deterministic = True\n","\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    seed_torch(seed)\n","\n","\n","seed = 0\n","set_seed(seed)\n","env = IZenv()\n","\n","num_steps = 6_000_000\n","agent = DQNAgent(\n","    env,\n","    device=\"cpu\",\n","    model_name=\"r2\",\n","    memory_size=1_000_000,\n","    batch_size=128,\n","    gamma=0.99,\n","    alpha=0.2,\n","    beta=0.6,\n","    prior_eps=1e-6,\n","    v_min=-6,\n","    v_max=55,\n","    atom_size=None,\n","    n_step=3,\n","    lr=1e-3,\n",")\n","# agent.load(\"model/r1_2023.11.19_00.55.17/42.5m.pth\")\n","# manually_test_agent(agent, fix_rand=False)\n","agent.train(\n","    update_target_every=2000,\n","    update_main_every=32,\n","    num_steps=num_steps,\n","    print_stats_every=10_000,\n","    save_every=100_000,\n","    eval_every=200_000,\n",")\n","evaluate_agent(agent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YEpc2RLQxyn","outputId":"f82d4638-f19b-44af-875c-410cf4c4e470","executionInfo":{"status":"ok","timestamp":1700866302293,"user_tz":480,"elapsed":914186,"user":{"displayName":"Mengfei Qi","userId":"03246893569933282016"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device.\n","Sp 10k/200k Mean losses 0.38 Mean winning sun 446.59 Mean steps 161.73 Mean score -2.28 Win 36.67% 3763.90s/1m steps\n","Sp 20k/200k Mean losses 0.34 Mean winning sun 456.45 Mean steps 248.70 Mean score -1.71 Win 38.75% 3777.33s/1m steps\n","Sp 30k/200k Mean losses 0.34 Mean winning sun 461.76 Mean steps 300.63 Mean score -2.49 Win 35.05% 3793.54s/1m steps\n","Sp 40k/200k Mean losses 0.33 Mean winning sun 481.55 Mean steps 345.71 Mean score -1.89 Win 36.52% 3813.20s/1m steps\n","Sp 50k/200k Mean losses 0.33 Mean winning sun 519.34 Mean steps 354.43 Mean score -1.04 Win 37.59% 3818.32s/1m steps\n","Sp 60k/200k Mean losses 0.32 Mean winning sun 492.50 Mean steps 378.54 Mean score -1.27 Win 38.22% 3823.72s/1m steps\n","Sp 70k/200k Mean losses 0.32 Mean winning sun 500.00 Mean steps 393.44 Mean score -0.55 Win 40.68% 3816.27s/1m steps\n","Sp 80k/200k Mean losses 0.31 Mean winning sun 499.68 Mean steps 403.77 Mean score -0.86 Win 39.39% 3820.01s/1m steps\n","Sp 90k/200k Mean losses 0.31 Mean winning sun 506.85 Mean steps 416.19 Mean score -0.86 Win 38.89% 3825.47s/1m steps\n","Sp 100k/200k Mean losses 0.30 Mean winning sun 550.26 Mean steps 409.08 Mean score -0.16 Win 38.93% 3840.33s/1m steps\n","Sp 110k/200k Mean losses 0.30 Mean winning sun 582.39 Mean steps 400.92 Mean score 1.24 Win 42.12% 3848.03s/1m steps\n","Sp 120k/200k Mean losses 0.30 Mean winning sun 651.22 Mean steps 369.07 Mean score 3.10 Win 44.31% 3848.60s/1m steps\n","Sp 130k/200k Mean losses 0.29 Mean winning sun 705.81 Mean steps 362.14 Mean score 5.29 Win 48.04% 3850.80s/1m steps\n","Sp 140k/200k Mean losses 0.29 Mean winning sun 747.75 Mean steps 351.95 Mean score 6.90 Win 50.38% 3860.39s/1m steps\n","Sp 150k/200k Mean losses 0.28 Mean winning sun 774.56 Mean steps 346.62 Mean score 8.30 Win 52.78% 3866.90s/1m steps\n","Sp 160k/200k Mean losses 0.28 Mean winning sun 796.60 Mean steps 339.00 Mean score 9.42 Win 54.56% 3871.16s/1m steps\n","Sp 170k/200k Mean losses 0.28 Mean winning sun 813.33 Mean steps 332.44 Mean score 10.24 Win 55.77% 3880.33s/1m steps\n","Sp 180k/200k Mean losses 0.28 Mean winning sun 828.59 Mean steps 328.55 Mean score 11.12 Win 57.22% 3883.04s/1m steps\n","Sp 190k/200k Mean losses 0.28 Mean winning sun 842.31 Mean steps 323.27 Mean score 11.58 Win 57.58% 3888.98s/1m steps\n","Sp 200k/200k Mean losses 0.28 Mean winning sun 852.88 Mean steps 322.91 Mean score 12.33 Win 58.90% 3898.78s/1m steps\n","Testing 500/500...\n","WIN: 376/500 (75.20%)\n","LOSE: 124/500 (24.80%)\n","Mean steps: 296.84\n","Mean winning suns: 998.14\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.752, 998.1382978723404)"]},"metadata":{},"execution_count":48}]}]}