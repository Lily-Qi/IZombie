{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwAQ6J6f4Gp4",
    "outputId": "7c7747d6-e220-4ad5-f141-85e39943013f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting git+https://github.com/Rottenham/PvZ-Emulator\n",
      "  Cloning https://github.com/Rottenham/PvZ-Emulator to /tmp/pip-req-build-kcvcv2lo\n",
      "  Running command git clone -q https://github.com/Rottenham/PvZ-Emulator /tmp/pip-req-build-kcvcv2lo\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Building wheels for collected packages: pvzemu\n",
      "  Building wheel for pvzemu (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pvzemu: filename=pvzemu-0.9.0-cp38-cp38-linux_x86_64.whl size=8608092 sha256=6bdafffa5976e4e3e056b9b66c1f169cf266a6bbb809b99b1331de8e2d933cd4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v14kkzfa/wheels/3c/6a/64/ce5b99cdd30871a09ed6847bc5d00d97c69874d8c6c53d062b\n",
      "Successfully built pvzemu\n",
      "Installing collected packages: pvzemu\n",
      "Successfully installed pvzemu-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT! First, go to Runtime -> Change runtime type, select a GPU runtime\n",
    "# And then Runtime -> Run all\n",
    "!pip install git+https://github.com/Rottenham/PvZ-Emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXwXYckvViOv",
    "tags": []
   },
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aM2hTDsUp7i"
   },
   "source": [
    "**config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O9qNPjfA4QmF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class GameStatus(Enum):\n",
    "    CONTINUE = 0\n",
    "    WIN = 1\n",
    "    LOSE = 2\n",
    "    TIMEUP = 3\n",
    "\n",
    "\n",
    "N_LANES = 5  # Height\n",
    "LANE_LENGTH = 9  # Width\n",
    "P_LANE_LENGTH = 4\n",
    "Z_LANE_LENGTH = LANE_LENGTH - P_LANE_LENGTH\n",
    "N_PLANT_TYPE = 4\n",
    "N_ZOMBIE_TYPE = 3\n",
    "SUN_MAX = 1950\n",
    "\n",
    "# action\n",
    "ACTION_SIZE = N_ZOMBIE_TYPE * N_LANES * Z_LANE_LENGTH + 1\n",
    "\n",
    "# state\n",
    "NUM_ZOMBIES = 39\n",
    "NUM_PLANTS = 20\n",
    "ZOMBIE_SIZE = 6\n",
    "PLANT_SIZE = 4\n",
    "BRAIN_BASE = NUM_ZOMBIES * ZOMBIE_SIZE + NUM_PLANTS * PLANT_SIZE + 1  # extra 1 for sun\n",
    "BRAIN_SIZE = 5\n",
    "STATE_SIZE = BRAIN_BASE + BRAIN_SIZE + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rru16G2zUsKq"
   },
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aAwHz-3D4SGV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pvzemu import (\n",
    "    World,\n",
    "    SceneType,\n",
    "    ZombieType,\n",
    "    PlantType,\n",
    "    IZObservation,\n",
    ")\n",
    "\n",
    "plant_counts = {\n",
    "    PlantType.sunflower: 9,\n",
    "    PlantType.pea_shooter: 6,\n",
    "    PlantType.squash: 3,\n",
    "    PlantType.snow_pea: 2,\n",
    "}\n",
    "\n",
    "zombie_deck = [\n",
    "    [ZombieType.zombie, 50],\n",
    "    [ZombieType.buckethead, 125],\n",
    "    [ZombieType.football, 175],\n",
    "]\n",
    "\n",
    "\n",
    "class IZenv:\n",
    "    def __init__(self, step_length=50, max_step=None):\n",
    "        self.step_length = step_length\n",
    "        self.max_step = max_step\n",
    "\n",
    "        self.ob_factory = IZObservation(NUM_ZOMBIES, NUM_PLANTS)\n",
    "        self.state = []\n",
    "        self.zombie_count, self.plant_count, self.brains = 0, 0, [0, 1, 2, 3, 4]\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.sun_spent = 0\n",
    "        self.world = World(SceneType.night)\n",
    "        self._reset_world()\n",
    "\n",
    "    def reset(self):\n",
    "        self.zombie_count, self.plant_count, self.brains = 0, 0, [0, 1, 2, 3, 4]\n",
    "        self.step_count = 0\n",
    "        self.sun_spent = 0\n",
    "        self._reset_world()\n",
    "        return self.get_state_and_mask()\n",
    "\n",
    "    def get_state_and_mask(self):\n",
    "        return self.state, self.get_action_mask()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        prev = {\n",
    "            \"sun_before_action\": self.get_sun(),\n",
    "            \"zombie_count\": self.zombie_count,\n",
    "            \"plant_count\": self.plant_count,\n",
    "            \"brain_count\": len(self.brains),\n",
    "            \"sun_spent_before_action\": self.sun_spent,\n",
    "            \"state\": self.state,\n",
    "        }\n",
    "\n",
    "        self._take_action(action)\n",
    "        prev[\"sun_after_action\"] = self.get_sun()\n",
    "\n",
    "        for _ in range(self.step_length):\n",
    "            self.world.update()\n",
    "        self._update_state()\n",
    "\n",
    "        game_status = self._get_game_status()\n",
    "        return (\n",
    "            self._get_reward(prev, action, game_status),\n",
    "            self.state,\n",
    "            self.get_action_mask(),\n",
    "            game_status,\n",
    "        )\n",
    "\n",
    "    def get_valid_actions(self, action_mask):\n",
    "        actions = np.arange(ACTION_SIZE)\n",
    "        return actions[action_mask]\n",
    "\n",
    "    def get_action_mask(self):\n",
    "        sun = self.get_sun()\n",
    "        mask = np.zeros(ACTION_SIZE, dtype=bool)\n",
    "        if self.zombie_count > 0:\n",
    "            mask[0] = True\n",
    "        if sun >= 50:\n",
    "            mask[1:26] = True\n",
    "        if sun >= 125:\n",
    "            mask[26:51] = True\n",
    "        if sun >= 175:\n",
    "            mask[51:] = True\n",
    "        return mask\n",
    "\n",
    "    def get_sun(self):\n",
    "        return self.world.scene.sun.sun\n",
    "\n",
    "    def _get_game_status(self):\n",
    "        if self.max_step is not None and self.step_count >= self.max_step:\n",
    "            return GameStatus.TIMEUP\n",
    "        if len(self.brains) == 0:\n",
    "            return GameStatus.WIN\n",
    "        if self.get_sun() < 50 and self.zombie_count == 0:\n",
    "            return GameStatus.LOSE\n",
    "        return GameStatus.CONTINUE\n",
    "\n",
    "    def _get_reward_pen_for_sun(self, prev, game_status):\n",
    "        if game_status == GameStatus.LOSE:\n",
    "            return -72\n",
    "\n",
    "        earned_sun = self.get_sun() - prev[\"sun_after_action\"]\n",
    "        spent_sun = prev[\"sun_before_action\"] - prev[\"sun_after_action\"]\n",
    "\n",
    "        reward = (\n",
    "            earned_sun - spent_sun * (1.001 ** prev[\"sun_spent_before_action\"])\n",
    "        ) / 25\n",
    "        if game_status == GameStatus.WIN:\n",
    "            reward += self.get_sun() / 25\n",
    "        return reward\n",
    "\n",
    "    def _get_reward_plain(self, prev, game_status):\n",
    "        # if game_status == GameStatus.LOSE:\n",
    "        #     return 0\n",
    "        return (self.get_sun() - prev[\"sun_before_action\"]) / 25\n",
    "\n",
    "    def _get_reward(self, prev, action, game_status):\n",
    "        # return self._get_reward_plain(prev, game_status)\n",
    "\n",
    "        # if game_status == GameStatus.LOSE:\n",
    "        #     return -72\n",
    "        # prev_state = prev[\"state\"]\n",
    "        eaten_plant_num = prev[\"plant_count\"] - self.plant_count\n",
    "        eaten_brain_num = prev[\"brain_count\"] - len(self.brains)\n",
    "        # zombie_count = self.zombie_count\n",
    "        reward = (self.get_sun() - prev[\"sun_before_action\"]) / 25\n",
    "        reward += eaten_brain_num * 8\n",
    "        reward += eaten_plant_num * 2\n",
    "        # for i in range(NUM_ZOMBIES):\n",
    "        #     base = i * ZOMBIE_SIZE\n",
    "        #     if self.state[base] - prev_state[base] > 0:\n",
    "        #       plant_count = 0\n",
    "        #       row = int(round(self.state[base + 2] * 5))\n",
    "        #       for j in range(NUM_PLANTS):\n",
    "        #         if self.state[base] != 0:\n",
    "        #           plant_row = int(round(self.state[base + 2] * 5))\n",
    "        #           if row == plant_row:\n",
    "        #             plant_count+=1\n",
    "        #       if plant_count <= 2:\n",
    "        #         reward -= (2-plant_count) * self.state[base]\n",
    "        if game_status == GameStatus.LOSE:\n",
    "            reward -= 5\n",
    "        return reward\n",
    "\n",
    "    def _reset_world(self) -> None:\n",
    "        self.world = World(SceneType.night)\n",
    "        self.world.scene.stop_spawn = True\n",
    "        self.world.scene.is_iz = True\n",
    "        self.world.scene.set_sun(150)\n",
    "        plant_list = [\n",
    "            plant for plant, count in plant_counts.items() for _ in range(count)\n",
    "        ]\n",
    "        np.random.shuffle(plant_list)\n",
    "        for index, plant in enumerate(plant_list):\n",
    "            self.world.plant_factory.create(\n",
    "                plant,\n",
    "                index // P_LANE_LENGTH,\n",
    "                index % P_LANE_LENGTH,\n",
    "            )\n",
    "        self._update_state()\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        if action > 0:\n",
    "            action -= 1\n",
    "            z_idx = action//(N_LANES * Z_LANE_LENGTH)\n",
    "            action_area = action%(N_LANES * Z_LANE_LENGTH)\n",
    "            row = action_area//N_LANES\n",
    "            col = action_area % N_LANES + 4\n",
    "            sun = self.get_sun() - zombie_deck[z_idx][1]\n",
    "            assert sun >= 0\n",
    "            self.world.zombie_factory.create(zombie_deck[z_idx][0], row, col)\n",
    "            self.world.scene.set_sun(sun)\n",
    "\n",
    "    def _update_state(self):\n",
    "        self.state, self.zombie_count, self.plant_count = self.ob_factory.create(\n",
    "            self.world\n",
    "        )\n",
    "        self.state.append(self.sun_spent / 1950)\n",
    "\n",
    "        self.brains = []\n",
    "        for i, b in enumerate(self.state[BRAIN_BASE : BRAIN_BASE + 5]):\n",
    "            if b > 0.5:\n",
    "                self.brains.append(i)\n",
    "\n",
    "    def print_human_readable_state(self, highlight=None):\n",
    "        def plant_str(plant_type):\n",
    "            if plant_type == 1:\n",
    "                return \"sun\"\n",
    "            if plant_type == 2:\n",
    "                return \"pea\"\n",
    "            if plant_type == 3:\n",
    "                return \"sqa\"\n",
    "            if plant_type == 4:\n",
    "                return \"sno\"\n",
    "            return \"---\"\n",
    "\n",
    "        def zombie_x_to_col(x):\n",
    "            col = int((x + 40) // 80)\n",
    "            return min(max(col, 0), LANE_LENGTH - 1)\n",
    "\n",
    "        def zombie_str(zombie_type):\n",
    "            if zombie_type == 1:\n",
    "                return \"Z\"\n",
    "            if zombie_type == 2:\n",
    "                return \"B\"\n",
    "            if zombie_type == 3:\n",
    "                return \"F\"\n",
    "            return \".\"\n",
    "\n",
    "        def acc1_hp_max(zombie_type):\n",
    "            if zombie_type == 2:\n",
    "                return 1100\n",
    "            if zombie_type == 3:\n",
    "                return 1400\n",
    "            return 0\n",
    "\n",
    "        plant_hps = [[0 for _ in range(P_LANE_LENGTH)] for _ in range(N_LANES)]\n",
    "        plant_types = [[\"---\" for _ in range(P_LANE_LENGTH)] for _ in range(N_LANES)]\n",
    "        zombie_hps = [[0 for _ in range(LANE_LENGTH)] for _ in range(N_LANES)]\n",
    "        zombie_types = [[\".\" for _ in range(LANE_LENGTH)] for _ in range(N_LANES)]\n",
    "\n",
    "        state = self.state\n",
    "\n",
    "        for i in range(NUM_PLANTS):\n",
    "            base = NUM_ZOMBIES * ZOMBIE_SIZE + i * PLANT_SIZE\n",
    "            if state[base] != 0:\n",
    "                plant_type = int(round(state[base] * 4))\n",
    "                hp = state[base + 1]\n",
    "                row = int(round(state[base + 2] * 5))\n",
    "                col = int(round(state[base + 3] * 9))\n",
    "\n",
    "                plant_hps[row][col] += hp * 300\n",
    "                plant_types[row][col] = plant_str(plant_type)\n",
    "\n",
    "        for i in range(NUM_ZOMBIES):\n",
    "            base = i * ZOMBIE_SIZE\n",
    "            if state[base] != 0:\n",
    "                zombie_type = int(round(state[base] * 3))\n",
    "                x = state[base + 1] * 650\n",
    "                row = int(round(state[base + 2] * 5))\n",
    "                hp = state[base + 3]\n",
    "                acc1_hp = state[base + 4]\n",
    "                col = zombie_x_to_col(x)\n",
    "\n",
    "                zombie_hps[row][col] += hp * 270 + acc1_hp * acc1_hp_max(zombie_type)\n",
    "                zombie_types[row][col] = zombie_str(zombie_type)\n",
    "\n",
    "        print(\"==Plant HP==\")\n",
    "        for row in range(N_LANES):\n",
    "            print(f\"row {row+1}: \", end=\"\")\n",
    "            for col in range(P_LANE_LENGTH):\n",
    "                print(f\"{plant_hps[row][col]:.2f}\\t\", end=\"\")\n",
    "            print()\n",
    "\n",
    "        print(\"==Plant Type==\")\n",
    "        for row in range(N_LANES):\n",
    "            print(f\"row {row+1}: \", end=\"\")\n",
    "            for col in range(P_LANE_LENGTH):\n",
    "                print(f\"{plant_types[row][col]}\\t\", end=\"\")\n",
    "            print()\n",
    "\n",
    "        print(\"==Zombie HP==\")\n",
    "        for row in range(N_LANES):\n",
    "            print(f\"row {row+1}: \", end=\"\")\n",
    "            for col in range(LANE_LENGTH):\n",
    "                print(f\"{zombie_hps[row][col]:.2f}\\t\", end=\"\")\n",
    "            print()\n",
    "\n",
    "        print(\"==Zombie Type==\")\n",
    "        highlight_row, highlight_col = (-1, -1) if highlight is None else highlight\n",
    "        for row in range(N_LANES):\n",
    "            print(f\"row {row+1}: \", end=\"\")\n",
    "            for col in range(LANE_LENGTH):\n",
    "                out = f\"{zombie_types[row][col]}\"\n",
    "                if row == highlight_row and col == highlight_col:\n",
    "                    out = f\"[{out}]\"\n",
    "                out += \"\\t\"\n",
    "                print(out, end=\"\")\n",
    "            print()\n",
    "\n",
    "        print(\n",
    "            f\"Step: {self.step_count}; Sun: {self.get_sun()}; Brains: {len(self.brains)}; Game status: {self._get_game_status().name} \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwuPV_EGVr1U",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3Mk7Kq1V3v-"
   },
   "source": [
    "Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gGV02Lfx4TqN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "class Epsilons:\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length,\n",
    "        start_epsilon,\n",
    "        end_epsilon,\n",
    "        interpolation=\"exponential\",\n",
    "    ):\n",
    "        self.seq_length = seq_length\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.end_epsilon = end_epsilon\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        assert seq_length > 1, \"seq_length must be positive\"\n",
    "        assert interpolation in [\"exponential\"], \"not implemented\"\n",
    "\n",
    "        self.index = 0\n",
    "        self.decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        self.has_warned = False\n",
    "\n",
    "    def get(self):\n",
    "        return self.start_epsilon * self.decay_rate**self.index\n",
    "\n",
    "    def next(self):\n",
    "        self.index += 1\n",
    "        if self.index >= self.seq_length:\n",
    "            if not self.has_warned:\n",
    "                self.has_warned = True\n",
    "                warnings.warn(\n",
    "                    f\"index = {self.index} overflows for seq_length = {self.seq_length}, using index = {self.seq_length - 1}.\"\n",
    "                )\n",
    "            self.index = self.seq_length - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LufQ8NlaXHsg"
   },
   "source": [
    "Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "w7Uuf0lD4Uod",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, test_size=500, step_count=None, output_file=None):\n",
    "    agent.set_to_eval_mode()\n",
    "    game_results = []\n",
    "    steps = []\n",
    "    winning_suns = []\n",
    "\n",
    "    for test_idx in range(1, test_size + 1):\n",
    "        print(f\"\\rTesting {test_idx}/{test_size}...\", end=\"\")\n",
    "        env = IZenv()\n",
    "        state, mask = env.get_state_and_mask()\n",
    "\n",
    "        for step in range(1_000_000):\n",
    "            action = agent.get_best_q_action(state, env.get_valid_actions(mask))\n",
    "            _, next_state, next_mask, game_status = env.step(action)\n",
    "            state, mask = next_state, next_mask\n",
    "\n",
    "            if game_status != GameStatus.CONTINUE:\n",
    "                game_results.append(game_status)\n",
    "                steps.append(step)\n",
    "                if game_status == GameStatus.WIN:\n",
    "                    winning_suns.append(env.get_sun())\n",
    "                break\n",
    "\n",
    "    print()\n",
    "\n",
    "    results_counter = Counter(game_results)\n",
    "    agent.set_to_training_mode()\n",
    "    return (np.mean(winning_suns), results_counter.get(GameStatus.WIN, 0) / len(game_results))\n",
    "\n",
    "\n",
    "def manually_test_agent(agent, fix_rand=True):\n",
    "    agent.set_to_eval_mode()\n",
    "\n",
    "    if fix_rand:\n",
    "        np.random.seed(0)\n",
    "    else:\n",
    "        np.random.seed()\n",
    "\n",
    "    env = IZenv()\n",
    "    state, mask = env.get_state_and_mask()\n",
    "    last_step = 0\n",
    "\n",
    "    for step in range(10000):\n",
    "        action = agent.get_best_q_action(state, env.get_valid_actions(mask))\n",
    "        reward, next_state, next_mask, game_status = env.step(action)\n",
    "\n",
    "        if action != 0 or game_status != GameStatus.CONTINUE:\n",
    "            env.print_human_readable_state(\n",
    "                highlight=((action - 1) % 5, 4) if action != 0 else None\n",
    "            )\n",
    "            print(f\"Action: {action}, Reward: {reward}, Î”Step: {step - last_step}\")\n",
    "            last_step = step\n",
    "            _ = input(\"\")\n",
    "\n",
    "        state, mask = next_state, next_mask\n",
    "\n",
    "        if game_status != GameStatus.CONTINUE:\n",
    "            break\n",
    "\n",
    "    agent.set_to_training_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1b2cJvFXSyy"
   },
   "source": [
    "Noisy Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cd0wnuOBXSYt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear module for NoisyNet.\n",
    "\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "        in_features (int): input size of linear module\n",
    "        out_features (int): output size of linear module\n",
    "        std_init (float): initial std value\n",
    "        weight_mu (nn.Parameter): mean value weight parameter\n",
    "        weight_sigma (nn.Parameter): std value weight parameter\n",
    "        bias_mu (nn.Parameter): mean value bias parameter\n",
    "        bias_sigma (nn.Parameter): std value bias parameter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        std_init: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(NoisyLinear, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.Tensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Make new noise.\"\"\"\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "\n",
    "        # outer product\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\n",
    "\n",
    "        We don't use separate statements on train / eval mode.\n",
    "        It doesn't show remarkable difference of performance.\n",
    "        \"\"\"\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_noise(size: int) -> torch.Tensor:\n",
    "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
    "        x = torch.randn(size)\n",
    "\n",
    "        return x.sign().mul(x.abs().sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRsEA7Y7XkiN"
   },
   "source": [
    "Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_O92wAQaXnSz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
    "\n",
    "\n",
    "def create_folder_if_not_exist(folder_name):\n",
    "    current_directory = os.getcwd()\n",
    "    folder_path = os.path.join(current_directory, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "\n",
    "def format_num(n):\n",
    "    if n > 1_000_000:\n",
    "        if n % 1_000_000 == 0:\n",
    "            return f\"{int(n / 1_000_000)}m\"\n",
    "        return f\"{n / 1_000_000}m\"\n",
    "    if n > 1_000:\n",
    "        if n % 1_000 == 0:\n",
    "            return f\"{int(n / 1_000)}k\"\n",
    "        return f\"{n / 1_000}k\"\n",
    "    return str(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fbiTTYKXqUT"
   },
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5Gk1qbGFXtxz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 23 15:25:15 2021\n",
    "\n",
    "@author: Lukas Frank\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "class Threshold:\n",
    "    \"\"\"\n",
    "    Generate sequences of epsilon thresholds.\n",
    "\n",
    "    :param seq_length: int, length of epsilon sequence = number of epsilons to draw\n",
    "\n",
    "    :param start_epsilon: float, value to start with\n",
    "\n",
    "    :param end_epsilon (optional): float, value to end with. If None, return constant\n",
    "        sequence of value start_epsilon. Default: None.\n",
    "\n",
    "    :param interpolation (optional): string, interpolation method:\\n\n",
    "        either 'linear', 'exponential' or 'sinusoidal'. Default: 'linear'.\n",
    "        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf\n",
    "\n",
    "    :param periods: int, number of periods for sinusoidal sequence. Default: 10.\n",
    "        ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length,\n",
    "        start_epsilon,\n",
    "        end_epsilon=None,\n",
    "        interpolation=\"linear\",\n",
    "        periods=10,\n",
    "    ):\n",
    "        self.seq_length = seq_length\n",
    "        self.start_epsilon = start_epsilon\n",
    "        assert interpolation in [\n",
    "            \"linear\",\n",
    "            \"exponential\",\n",
    "            \"sinusoidal\",\n",
    "        ], \"interpolation argument invalid. Must be 'linear', 'exponential', 'sinusoidal' or unspecified.\"\n",
    "        self.interpolation = interpolation\n",
    "        if end_epsilon is None:\n",
    "            self.end_epsilon = start_epsilon\n",
    "            # set to linear to deliver constant sequence of epsilons.\n",
    "            self.interpolation = \"linear\"\n",
    "        else:\n",
    "            self.end_epsilon = end_epsilon\n",
    "        self.periods = periods\n",
    "\n",
    "    def epsilon(self, index=None):\n",
    "        \"\"\"Return sequence or element of sequence of epsilons as specified\n",
    "\n",
    "        :param index (optional): index of sequence element to be returned. If None, return\n",
    "            full sequence. Default: None.\\n\n",
    "\n",
    "        :return: array-like with shape (self.seq_length) or a single float value.\n",
    "        \"\"\"\n",
    "        epsilon = None\n",
    "\n",
    "        if self.interpolation == \"linear\":\n",
    "            epsilon = self._linear(index)\n",
    "\n",
    "        elif self.interpolation == \"exponential\":\n",
    "            epsilon = self._exponential(index)\n",
    "\n",
    "        elif self.interpolation == \"sinusoidal\":\n",
    "            epsilon = self._sinusoidal(index, self.periods)\n",
    "\n",
    "        return epsilon\n",
    "\n",
    "    def _linear(self, index):\n",
    "        \"\"\"Calls linear calculation method depending on whether index is given or not.\"\"\"\n",
    "        if index is not None:  # return only one epsilon\n",
    "            self._check_index_length(index)\n",
    "            return self._linear_point(index)\n",
    "        else:\n",
    "            return self._linear_sequence()\n",
    "\n",
    "    def _exponential(self, index):\n",
    "        \"\"\"Calls exponential calculation depending on whether index is given or not.\"\"\"\n",
    "        if index is not None:  # return only one epsilon\n",
    "            self._check_index_length(index)\n",
    "            return self._exponential_point(index)\n",
    "        else:\n",
    "            return self._exponential_sequence()\n",
    "\n",
    "    def _sinusoidal(self, index, periods):\n",
    "        \"\"\"Calls sinusoidal calculation depending on whether index is given or not.\"\"\"\n",
    "        if index is not None:  # return only one epsilon\n",
    "            self._check_index_length(index)\n",
    "            return self._sinusoidal_point(index, mini_epochs=periods)\n",
    "        else:\n",
    "            return self._sinusoidal_sequence(mini_epochs=periods)\n",
    "\n",
    "    def _linear_sequence(self):\n",
    "        \"\"\"Computes linear sequence\"\"\"\n",
    "        return np.linspace(\n",
    "            start=self.start_epsilon, stop=self.end_epsilon, num=self.seq_length\n",
    "        ).tolist()\n",
    "\n",
    "    def _linear_point(self, index):\n",
    "        \"\"\"Computes a single point by linear interpolation\"\"\"\n",
    "        return (\n",
    "            self.start_epsilon\n",
    "            + (self.end_epsilon - self.start_epsilon) / (self.seq_length - 1) * index\n",
    "        )\n",
    "\n",
    "    def _exponential_sequence(self):\n",
    "        \"\"\"Computes exponential sequence\"\"\"\n",
    "        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        return [(self.start_epsilon * decay_rate**i) for i in range(self.seq_length)]\n",
    "\n",
    "    def _exponential_point(self, index):\n",
    "        \"\"\"Computes a single point by exponential interpolation\"\"\"\n",
    "        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        return self.start_epsilon * decay_rate**index\n",
    "\n",
    "    def _sinusoidal_sequence(self, mini_epochs):\n",
    "        \"\"\"Computes sinusoidal sequence.\n",
    "\n",
    "        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf \\n\n",
    "\n",
    "        :param mini_epochs (optional): int, number of oscillations in sequence.\n",
    "        \"\"\"\n",
    "        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        return [\n",
    "            (\n",
    "                self.start_epsilon\n",
    "                * decay_rate**i\n",
    "                * 0.5\n",
    "                * (1 + np.cos(2 * math.pi * i * mini_epochs / (self.seq_length - 1)))\n",
    "            )\n",
    "            for i in range(self.seq_length)\n",
    "        ]\n",
    "\n",
    "    def _sinusoidal_point(self, index, mini_epochs):\n",
    "        \"\"\"Computes a single point by sinusoidal interpolation.\n",
    "\n",
    "        Reference: http://cs231n.stanford.edu/reports/2017/pdfs/616.pdf \\n\n",
    "\n",
    "        :param mini_epochs (optional): int, number of oscillations in sequence.\n",
    "        \"\"\"\n",
    "        decay_rate = (self.end_epsilon / self.start_epsilon) ** (\n",
    "            1 / (self.seq_length - 1)\n",
    "        )\n",
    "        return (\n",
    "            self.start_epsilon\n",
    "            * decay_rate**index\n",
    "            * 0.5\n",
    "            * (1 + np.cos(2 * math.pi * index * mini_epochs / (self.seq_length - 1)))\n",
    "        )\n",
    "\n",
    "    def _check_index_length(self, index):\n",
    "        \"\"\"Check whether index is in sequence.\"\"\"\n",
    "        if index >= self.seq_length:\n",
    "            warnings.warn(\n",
    "                f\"threshold.epsilon(index): index {index} > seq_length {self.seq_length}.\"\n",
    "                f\"Changing index to {index - self.seq_length}.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RHmcNTjX5er"
   },
   "source": [
    "Segment Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F4d14OvUX7nk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Segment tree for Prioritized Replay Buffer.\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class SegmentTree:\n",
    "    \"\"\" Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int)\n",
    "        tree (list)\n",
    "        operation (function)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, operation: Callable, init_value: float):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "            operation (function)\n",
    "            init_value (float)\n",
    "\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            capacity > 0 and capacity & (capacity - 1) == 0\n",
    "        ), \"capacity must be positive and a power of 2.\"\n",
    "        self.capacity = capacity\n",
    "        self.tree = [init_value for _ in range(2 * capacity)]\n",
    "        self.operation = operation\n",
    "\n",
    "    def _operate_helper(\n",
    "        self, start: int, end: int, node: int, node_start: int, node_end: int\n",
    "    ) -> float:\n",
    "        \"\"\"Returns result of operation in segment.\"\"\"\n",
    "        if start == node_start and end == node_end:\n",
    "            return self.tree[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._operate_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._operate_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self.operation(\n",
    "                    self._operate_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._operate_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),\n",
    "                )\n",
    "\n",
    "    def operate(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns result of applying `self.operation`.\"\"\"\n",
    "        if end <= 0:\n",
    "            end += self.capacity\n",
    "        end -= 1\n",
    "\n",
    "        return self._operate_helper(start, end, 1, 0, self.capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx: int, val: float):\n",
    "        \"\"\"Set value in tree.\"\"\"\n",
    "        idx += self.capacity\n",
    "        self.tree[idx] = val\n",
    "\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self.tree[idx] = self.operation(self.tree[2 * idx], self.tree[2 * idx + 1])\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx: int) -> float:\n",
    "        \"\"\"Get real value in leaf node of tree.\"\"\"\n",
    "        assert 0 <= idx < self.capacity\n",
    "\n",
    "        return self.tree[self.capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    \"\"\" Create SumSegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=operator.add, init_value=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns arr[start] + ... + arr[end].\"\"\"\n",
    "        return super(SumSegmentTree, self).operate(start, end)\n",
    "\n",
    "    def retrieve(self, upperbound: float) -> int:\n",
    "        \"\"\"Find the highest index `i` about upper bound in the tree\"\"\"\n",
    "        # TODO: Check assert case and fix bug\n",
    "        assert 0 <= upperbound <= self.sum() + 1e-5, \"upperbound: {}\".format(upperbound)\n",
    "\n",
    "        idx = 1\n",
    "\n",
    "        while idx < self.capacity:  # while non-leaf\n",
    "            left = 2 * idx\n",
    "            right = left + 1\n",
    "            if self.tree[left] > upperbound:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                upperbound -= self.tree[left]\n",
    "                idx = right\n",
    "        return idx - self.capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    \"\"\" Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=min, init_value=float(\"inf\")\n",
    "        )\n",
    "\n",
    "    def min(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end]).\"\"\"\n",
    "        return super(MinSegmentTree, self).operate(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-SJbQ7VX_jh"
   },
   "source": [
    "Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CC4-hF7bYB8K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import Tuple, Dict, Deque, List\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        size: int,\n",
    "        batch_size: int = 32,\n",
    "        n_step: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        (\n",
    "            self.ptr,\n",
    "            self.size,\n",
    "        ) = (\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        # for N-step Learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: np.ndarray,\n",
    "        rew: float,\n",
    "        next_obs: np.ndarray,\n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        transition = (obs, act, rew, next_obs, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        # single step transition is not ready\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return ()\n",
    "\n",
    "        # make a n-step transition\n",
    "        rew, next_obs, done = self._get_n_step_info(self.n_step_buffer, self.gamma)\n",
    "        obs, act = self.n_step_buffer[0][:2]\n",
    "\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        return self.n_step_buffer[0]\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "            # for N-step Learning\n",
    "            indices=idxs,\n",
    "        )\n",
    "\n",
    "    def sample_batch_from_idxs(self, idxs: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        # for N-step Learning\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "        )\n",
    "\n",
    "    def _get_n_step_info(\n",
    "        self, n_step_buffer: Deque, gamma: float\n",
    "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
    "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
    "        # info of the last transition\n",
    "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
    "\n",
    "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
    "            r, n_o, d = transition[-3:]\n",
    "\n",
    "            rew = r + gamma * rew * (1 - d)\n",
    "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
    "\n",
    "        return rew, next_obs, done\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    \"\"\"Prioritized Replay buffer.\n",
    "\n",
    "    Attributes:\n",
    "        max_priority (float): max priority\n",
    "        tree_ptr (int): next index of tree\n",
    "        alpha (float): alpha parameter for prioritized replay buffer\n",
    "        sum_tree (SumSegmentTree): sum tree for prior\n",
    "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        size: int,\n",
    "        batch_size: int = 32,\n",
    "        alpha: float = 0.6,\n",
    "        n_step: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        assert alpha >= 0\n",
    "\n",
    "        super(PrioritizedReplayBuffer, self).__init__(\n",
    "            obs_dim, size, batch_size, n_step, gamma\n",
    "        )\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: int,\n",
    "        rew: float,\n",
    "        next_obs: np.ndarray,\n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        \"\"\"Store experience and priority.\"\"\"\n",
    "        transition = super().store(obs, act, rew, next_obs, done)\n",
    "\n",
    "        if transition:\n",
    "            self.sum_tree[self.tree_ptr] = self.max_priority**self.alpha\n",
    "            self.min_tree[self.tree_ptr] = self.max_priority**self.alpha\n",
    "            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "\n",
    "        return transition\n",
    "\n",
    "    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "\n",
    "        indices = self._sample_proportional()\n",
    "\n",
    "        obs = self.obs_buf[indices]\n",
    "        next_obs = self.next_obs_buf[indices]\n",
    "        acts = self.acts_buf[indices]\n",
    "        rews = self.rews_buf[indices]\n",
    "        done = self.done_buf[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "\n",
    "        return dict(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            acts=acts,\n",
    "            rews=rews,\n",
    "            done=done,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
    "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority**self.alpha\n",
    "            self.min_tree[idx] = priority**self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        \"\"\"Sample indices based on proportions.\"\"\"\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment = p_total / self.batch_size\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def _calculate_weight(self, idx: int, beta: float):\n",
    "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "\n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IWB1PI1UQv54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim: int, out_dim: int, atom_size: int, support: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.support = support\n",
    "        self.out_dim = out_dim\n",
    "        self.atom_size = atom_size\n",
    "\n",
    "        # set common feature layer\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # set advantage layer\n",
    "        self.advantage_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.advantage_layer = NoisyLinear(128, out_dim * atom_size)\n",
    "\n",
    "        # set value layer\n",
    "        self.value_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.value_layer = NoisyLinear(128, atom_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        dist = self.dist(x)\n",
    "        q = torch.sum(dist * self.support, dim=2)\n",
    "\n",
    "        return q\n",
    "\n",
    "    def dist(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get distribution for atoms.\"\"\"\n",
    "        feature = self.feature_layer(x)\n",
    "        adv_hid = F.relu(self.advantage_hidden_layer(feature))\n",
    "        val_hid = F.relu(self.value_hidden_layer(feature))\n",
    "\n",
    "        advantage = self.advantage_layer(adv_hid).view(-1, self.out_dim, self.atom_size)\n",
    "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "        dist = dist.clamp(min=1e-3)  # for avoiding nans\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset all noisy layers.\"\"\"\n",
    "        self.advantage_hidden_layer.reset_noise()\n",
    "        self.advantage_layer.reset_noise()\n",
    "        self.value_hidden_layer.reset_noise()\n",
    "        self.value_layer.reset_noise()\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "\n",
    "    Attribute:\n",
    "        env (IZenv): izombie env\n",
    "        memory (PrioritizedReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including\n",
    "                           state, action, reward, next_state, done\n",
    "        v_min (float): min value of support\n",
    "        v_max (float): max value of support\n",
    "        atom_size (int): the unit number of support\n",
    "        support (torch.Tensor): support for categorical dqn\n",
    "        use_n_step (bool): whether to use n_step memory\n",
    "        n_step (int): step number to calculate n-step td error\n",
    "        memory_n (ReplayBuffer): n-step replay buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: IZenv,\n",
    "        model_name: str,\n",
    "        device: str,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-3,\n",
    "        # PER parameters\n",
    "        alpha: float = 0.2,\n",
    "        beta: float = 0.6,\n",
    "        prior_eps: float = 1e-6,\n",
    "        # Categorical DQN parameters\n",
    "        v_min: float = 0.0,\n",
    "        v_max: float = 200.0,\n",
    "        atom_size: int = 51,\n",
    "        # N-step Learning\n",
    "        n_step: int = 3,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            lr (float): learning rate\n",
    "            gamma (float): discount factor\n",
    "            alpha (float): determines how much prioritization is used\n",
    "            beta (float): determines how much importance sampling is used\n",
    "            prior_eps (float): guarantees every transition can be sampled\n",
    "            v_min (float): min value of support\n",
    "            v_max (float): max value of support\n",
    "            atom_size (int): the unit number of support\n",
    "            n_step (int): step number to calculate n-step td error\n",
    "        \"\"\"\n",
    "        if atom_size is None:\n",
    "            atom_size = v_max - v_min + 1\n",
    "        obs_dim = STATE_SIZE\n",
    "        action_dim = ACTION_SIZE\n",
    "\n",
    "        self.env = env\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        # NoisyNet: All attributes related to epsilon are removed\n",
    "\n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(device)\n",
    "        print(f\"Using {self.device} device.\")\n",
    "\n",
    "        # PER\n",
    "        # memory for 1-step Learning\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            obs_dim, memory_size, batch_size, alpha=alpha, gamma=gamma\n",
    "        )\n",
    "\n",
    "        # memory for N-step Learning\n",
    "        self.use_n_step = True if n_step > 1 else False\n",
    "        if self.use_n_step:\n",
    "            self.n_step = n_step\n",
    "            self.memory_n = ReplayBuffer(\n",
    "                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n",
    "            )\n",
    "\n",
    "        # Categorical DQN parameters\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.atom_size = atom_size\n",
    "        self.support = torch.linspace(self.v_min, self.v_max, self.atom_size).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(obs_dim, action_dim, self.atom_size, self.support).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.dqn_target = Network(obs_dim, action_dim, self.atom_size, self.support).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "\n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "        # stats\n",
    "        self.winning_suns = []\n",
    "        self.losses = []\n",
    "        self.game_results = []\n",
    "        self.steps = []\n",
    "        self.scores = []\n",
    "        self.stats_data = []\n",
    "\n",
    "    def get_best_q_action(self, state, mask):\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # NoisyNet: no epsilon greedy action selection\n",
    "        with torch.no_grad():\n",
    "            valid_actions = self.env.get_valid_actions(mask)\n",
    "            q_values = self.dqn(\n",
    "                torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            ).detach()\n",
    "            valid_q_values = q_values[0, valid_actions]\n",
    "            max_q_index = torch.argmax(valid_q_values).item()\n",
    "            selected_action = valid_actions[max_q_index]\n",
    "\n",
    "            if not self.is_test:\n",
    "                self.transition = [state, selected_action]\n",
    "\n",
    "            return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        reward, next_state, next_mask, game_status = self.env.step(action)\n",
    "        done = game_status != GameStatus.CONTINUE\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "\n",
    "            # N-step transition\n",
    "            if self.use_n_step:\n",
    "                one_step_transition = self.memory_n.store(*self.transition)\n",
    "            # 1-step transition\n",
    "            else:\n",
    "                one_step_transition = self.transition\n",
    "\n",
    "            # add a single step transition\n",
    "            if one_step_transition:\n",
    "                self.memory.store(*one_step_transition)\n",
    "\n",
    "        return next_state, next_mask, reward, game_status, done\n",
    "\n",
    "    def update_main_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        # PER needs beta to calculate weights\n",
    "        samples = self.memory.sample_batch(self.beta)\n",
    "        weights = torch.FloatTensor(samples[\"weights\"].reshape(-1, 1)).to(self.device)\n",
    "        indices = samples[\"indices\"]\n",
    "\n",
    "        # 1-step Learning loss\n",
    "        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n",
    "\n",
    "        # PER: importance sampling before average\n",
    "        loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        # N-step Learning loss\n",
    "        # we are gonna combine 1-step loss and n-step loss so as to\n",
    "        # prevent high-variance. The original rainbow employs n-step loss only.\n",
    "        if self.use_n_step:\n",
    "            gamma = self.gamma**self.n_step\n",
    "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
    "            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n",
    "            elementwise_loss += elementwise_loss_n_loss\n",
    "\n",
    "            # PER: importance sampling before average\n",
    "            loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.dqn.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # PER: update priorities\n",
    "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
    "        new_priorities = loss_for_prior + self.prior_eps\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "\n",
    "        # NoisyNet: reset noise\n",
    "        self.dqn.reset_noise()\n",
    "        self.dqn_target.reset_noise()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_steps: int,\n",
    "        stats_window: int = 1_000,\n",
    "        print_stats_every=30_000,\n",
    "        update_target_every=2000,\n",
    "        update_main_every=1,\n",
    "        save_every=None,\n",
    "        eval_every=None,\n",
    "    ):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        model_dir = f\"model/{self.model_name}_{get_timestamp()}\"\n",
    "        self.is_test = False\n",
    "        self.set_to_training_mode()\n",
    "\n",
    "        state, mask = self.env.reset()\n",
    "        start_time = datetime.datetime.now()\n",
    "        score = 0\n",
    "\n",
    "        for step_idx in range(1, num_steps + 1):\n",
    "            action = self.get_best_q_action(state, mask)\n",
    "            state, mask, reward, game_status, done = self.step(action)\n",
    "            score += reward\n",
    "\n",
    "            # NoisyNet: removed decrease of epsilon\n",
    "\n",
    "            # PER: increase beta\n",
    "            fraction = min(step_idx / num_steps, 1.0)\n",
    "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                self.game_results.append(game_status)\n",
    "                self.steps.append(self.env.step_count)\n",
    "                self.scores.append(score)\n",
    "                score = 0\n",
    "                if game_status == GameStatus.WIN:\n",
    "                    self.winning_suns.append(self.env.get_sun())\n",
    "                state, mask = self.env.reset()\n",
    "\n",
    "            # if training is ready\n",
    "            if (\n",
    "                len(self.memory) >= self.batch_size\n",
    "                and step_idx % update_main_every == 0\n",
    "            ):\n",
    "                loss = self.update_main_model()\n",
    "                self.losses.append(loss)\n",
    "\n",
    "            # if hard update is needed\n",
    "            if step_idx % update_target_every == 0:\n",
    "                self._sync_target_with_main()\n",
    "\n",
    "            if step_idx % print_stats_every == 0:\n",
    "                self.print_stats(stats_window, step_idx, num_steps, start_time)\n",
    "\n",
    "            if eval_every is not None and step_idx % eval_every == 0:\n",
    "                avg_suns, win_rate = evaluate_agent(self,step_count=step_idx)\n",
    "                self.stats_data.append((step_idx, win_rate, avg_suns))\n",
    "            if (\n",
    "                save_every is not None and step_idx % save_every == 0\n",
    "            ) or step_idx == num_steps:\n",
    "                self.save(f\"{model_dir}/{format_num(step_idx)}.pth\")\n",
    "\n",
    "    def _compute_dqn_loss(\n",
    "        self, samples: Dict[str, np.ndarray], gamma: float\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Return categorical dqn loss.\"\"\"\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(self.device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(self.device)\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(self.device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(self.device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(self.device)\n",
    "\n",
    "        # Categorical DQN algorithm\n",
    "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_action = self.dqn(next_state).argmax(1)\n",
    "            next_dist = self.dqn_target.dist(next_state)\n",
    "            next_dist = next_dist[range(self.batch_size), next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n",
    "                )\n",
    "                .long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.atom_size)\n",
    "                .to(self.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn.dist(state)\n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "\n",
    "        return elementwise_loss\n",
    "\n",
    "    def _sync_target_with_main(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def print_stats(self, stats_window, curr_step, total_step, start_time):\n",
    "        win_rate = (\n",
    "            sum(1 for res in self.game_results[-stats_window:] if res == GameStatus.WIN)\n",
    "            * 100\n",
    "            / min(stats_window, len(self.game_results))\n",
    "        )\n",
    "        elasped_seconds = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        print(\n",
    "            f\"Sp {format_num(curr_step)}/{format_num(total_step)} \"\n",
    "            f\"Mean losses {np.mean(self.losses[-stats_window:]):.2f} \"\n",
    "            f\"Mean winning sun {np.mean(self.winning_suns[-stats_window:]):.2f} \"\n",
    "            f\"Mean steps {np.mean(self.steps[-stats_window:]):.2f} \"\n",
    "            f\"Mean score {np.mean(self.scores[-stats_window:]):.2f} \"\n",
    "            f\"Win {win_rate:.2f}% \"\n",
    "            f\"{elasped_seconds / curr_step * 1_000_000:.2f}s/1m steps\"\n",
    "        )\n",
    "\n",
    "    def set_to_training_mode(self):\n",
    "        self.dqn.train()\n",
    "\n",
    "    def set_to_eval_mode(self):\n",
    "        self.dqn.eval()\n",
    "\n",
    "    def save(self, filename):\n",
    "        assert not os.path.exists(filename)\n",
    "        create_folder_if_not_exist(os.path.dirname(filename))\n",
    "        torch.save(self.dqn.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=self.device)\n",
    "        self.dqn.load_state_dict(state_dict)\n",
    "        self.dqn_target.load_state_dict(state_dict)\n",
    "    \n",
    "    def save_stats_to_csv(self, filename):\n",
    "        df = pd.DataFrame(self.stats_data, columns=['Step Count', 'Winning Rate', 'Average Remaining Suns'])\n",
    "        df.to_csv(filename, index=False)\n",
    "    \n",
    "    def plot_stats_in_one_graph(self, title):\n",
    "        # Extract data\n",
    "        episodes = [data[0] for data in self.stats_data]\n",
    "        win_rates = [data[1] * 100 for data in self.stats_data]\n",
    "        avg_suns = [data[2] for data in self.stats_data]\n",
    "\n",
    "        # Create subplots\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # Plot win rate\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('Step Count')\n",
    "        ax1.set_ylabel('Winning Rate (%)', color=color)\n",
    "        ax1.plot(episodes, win_rates, label=\"Winning Rate\", color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # Create a second y-axis for average suns\n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Average Remaining Suns', color=color) \n",
    "        ax2.plot(episodes, avg_suns, label=\"Average Remaining Suns\", color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout() \n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_stats_winning_rate(self, title):\n",
    "        # Extract data\n",
    "        episodes = [data[0] for data in self.stats_data]\n",
    "        win_rates = [data[1] * 100 for data in self.stats_data]\n",
    "\n",
    "        # Plot for Win Rate\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(episodes, win_rates, label=\"Winning Rate\", color='tab:red')\n",
    "        plt.xlabel(\"Step Count\")\n",
    "        plt.ylabel(\"Winning Rate (%)\")\n",
    "        plt.title(title)\n",
    "        # plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_stats_remaining_suns(self, title):\n",
    "        # Extract data\n",
    "        episodes = [data[0] for data in self.stats_data]\n",
    "        avg_suns = [data[2] for data in self.stats_data]\n",
    "\n",
    "        # Plot for Average Remaining Suns\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(episodes, avg_suns, label=\"Average Remaining Suns\", color='tab:blue')\n",
    "        plt.xlabel(\"Step Count\")\n",
    "        plt.ylabel(\"Average Remaining Suns\")\n",
    "        plt.title(title)\n",
    "        # plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbXmAC1IYzb7",
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3YEpc2RLQxyn",
    "outputId": "86718efd-1c54-41ff-8c2b-037ccb0f2521",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n",
      "Sp 10k/2m Mean losses 0.51 Mean winning sun 50.00 Mean steps 149.95 Mean score 14.89 Win 1.52% 2556.20s/1m steps\n",
      "Sp 20k/2m Mean losses 0.36 Mean winning sun 50.00 Mean steps 161.27 Mean score 15.64 Win 0.81% 2784.23s/1m steps\n",
      "Sp 30k/2m Mean losses 0.36 Mean winning sun 25.00 Mean steps 182.21 Mean score 15.35 Win 1.22% 2688.78s/1m steps\n",
      "Sp 40k/2m Mean losses 0.36 Mean winning sun 25.00 Mean steps 202.58 Mean score 16.97 Win 1.02% 2756.05s/1m steps\n",
      "Sp 50k/2m Mean losses 0.35 Mean winning sun 25.00 Mean steps 227.20 Mean score 18.29 Win 0.92% 2716.10s/1m steps\n",
      "Sp 60k/2m Mean losses 0.35 Mean winning sun 25.00 Mean steps 235.73 Mean score 18.70 Win 0.79% 2699.70s/1m steps\n",
      "Sp 70k/2m Mean losses 0.35 Mean winning sun 25.00 Mean steps 218.82 Mean score 18.68 Win 0.63% 2682.66s/1m steps\n",
      "Sp 80k/2m Mean losses 0.34 Mean winning sun 141.67 Mean steps 223.11 Mean score 19.60 Win 0.84% 2664.64s/1m steps\n",
      "Sp 90k/2m Mean losses 0.34 Mean winning sun 141.67 Mean steps 233.72 Mean score 19.82 Win 0.78% 2654.31s/1m steps\n",
      "Sp 100k/2m Mean losses 0.34 Mean winning sun 131.25 Mean steps 246.02 Mean score 20.40 Win 0.99% 2645.68s/1m steps\n",
      "Sp 110k/2m Mean losses 0.33 Mean winning sun 170.00 Mean steps 259.18 Mean score 20.83 Win 1.19% 2668.75s/1m steps\n",
      "Sp 120k/2m Mean losses 0.33 Mean winning sun 170.00 Mean steps 272.62 Mean score 21.14 Win 1.14% 2661.58s/1m steps\n",
      "Sp 130k/2m Mean losses 0.33 Mean winning sun 183.33 Mean steps 281.82 Mean score 21.31 Win 1.30% 2698.34s/1m steps\n",
      "Sp 140k/2m Mean losses 0.32 Mean winning sun 303.57 Mean steps 291.77 Mean score 21.67 Win 1.47% 2691.67s/1m steps\n",
      "Sp 150k/2m Mean losses 0.32 Mean winning sun 362.50 Mean steps 301.22 Mean score 22.17 Win 1.61% 2729.60s/1m steps\n",
      "Sp 160k/2m Mean losses 0.31 Mean winning sun 362.50 Mean steps 307.57 Mean score 22.46 Win 1.54% 2726.88s/1m steps\n",
      "Sp 170k/2m Mean losses 0.31 Mean winning sun 402.78 Mean steps 316.28 Mean score 22.77 Win 1.68% 2724.76s/1m steps\n",
      "Sp 180k/2m Mean losses 0.31 Mean winning sun 420.83 Mean steps 319.78 Mean score 22.94 Win 2.14% 2720.23s/1m steps\n",
      "Sp 190k/2m Mean losses 0.31 Mean winning sun 463.46 Mean steps 327.14 Mean score 23.40 Win 2.25% 2713.84s/1m steps\n",
      "Sp 200k/2m Mean losses 0.30 Mean winning sun 463.46 Mean steps 335.83 Mean score 23.51 Win 2.18% 2712.20s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 210k/2m Mean losses 0.30 Mean winning sun 430.00 Mean steps 345.05 Mean score 23.91 Win 2.47% 3714.39s/1m steps\n",
      "Sp 220k/2m Mean losses 0.30 Mean winning sun 430.00 Mean steps 354.69 Mean score 24.22 Win 2.42% 3671.53s/1m steps\n",
      "Sp 230k/2m Mean losses 0.29 Mean winning sun 430.00 Mean steps 363.35 Mean score 24.42 Win 2.38% 3631.16s/1m steps\n",
      "Sp 240k/2m Mean losses 0.29 Mean winning sun 430.00 Mean steps 372.20 Mean score 24.88 Win 2.33% 3589.69s/1m steps\n",
      "Sp 250k/2m Mean losses 0.29 Mean winning sun 464.06 Mean steps 377.95 Mean score 25.05 Win 2.42% 3553.86s/1m steps\n",
      "Sp 260k/2m Mean losses 0.29 Mean winning sun 506.94 Mean steps 382.91 Mean score 25.12 Win 2.65% 3518.49s/1m steps\n",
      "Sp 270k/2m Mean losses 0.28 Mean winning sun 579.17 Mean steps 382.83 Mean score 25.72 Win 3.41% 3485.46s/1m steps\n",
      "Sp 280k/2m Mean losses 0.28 Mean winning sun 588.00 Mean steps 385.97 Mean score 25.81 Win 3.45% 3458.11s/1m steps\n",
      "Sp 290k/2m Mean losses 0.28 Mean winning sun 588.00 Mean steps 390.65 Mean score 25.94 Win 3.37% 3431.01s/1m steps\n",
      "Sp 300k/2m Mean losses 0.28 Mean winning sun 567.31 Mean steps 395.59 Mean score 26.15 Win 3.44% 3407.51s/1m steps\n",
      "Sp 310k/2m Mean losses 0.27 Mean winning sun 564.81 Mean steps 401.46 Mean score 26.30 Win 3.50% 3383.33s/1m steps\n",
      "Sp 320k/2m Mean losses 0.27 Mean winning sun 564.81 Mean steps 404.84 Mean score 26.55 Win 3.42% 3362.21s/1m steps\n",
      "Sp 330k/2m Mean losses 0.27 Mean winning sun 564.29 Mean steps 405.61 Mean score 26.73 Win 3.44% 3342.66s/1m steps\n",
      "Sp 340k/2m Mean losses 0.27 Mean winning sun 564.29 Mean steps 406.06 Mean score 26.63 Win 3.35% 3323.82s/1m steps\n",
      "Sp 350k/2m Mean losses 0.28 Mean winning sun 586.21 Mean steps 408.67 Mean score 26.65 Win 3.39% 3306.06s/1m steps\n",
      "Sp 360k/2m Mean losses 0.28 Mean winning sun 567.50 Mean steps 412.86 Mean score 26.79 Win 3.45% 3288.18s/1m steps\n",
      "Sp 370k/2m Mean losses 0.28 Mean winning sun 566.13 Mean steps 417.40 Mean score 26.90 Win 3.50% 3271.12s/1m steps\n",
      "Sp 380k/2m Mean losses 0.28 Mean winning sun 580.15 Mean steps 416.12 Mean score 26.95 Win 3.74% 3256.12s/1m steps\n",
      "Sp 390k/2m Mean losses 0.28 Mean winning sun 545.93 Mean steps 417.82 Mean score 27.64 Win 4.61% 3241.90s/1m steps\n",
      "Sp 400k/2m Mean losses 0.27 Mean winning sun 543.18 Mean steps 421.94 Mean score 27.80 Win 4.64% 3228.67s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 410k/2m Mean losses 0.27 Mean winning sun 535.56 Mean steps 422.31 Mean score 27.86 Win 4.64% 3905.16s/1m steps\n",
      "Sp 420k/2m Mean losses 0.27 Mean winning sun 535.56 Mean steps 425.55 Mean score 27.88 Win 4.56% 3878.56s/1m steps\n",
      "Sp 430k/2m Mean losses 0.27 Mean winning sun 544.27 Mean steps 428.24 Mean score 28.14 Win 4.80% 3851.50s/1m steps\n",
      "Sp 440k/2m Mean losses 0.27 Mean winning sun 536.27 Mean steps 435.51 Mean score 28.49 Win 5.10% 3824.15s/1m steps\n",
      "Sp 450k/2m Mean losses 0.27 Mean winning sun 533.65 Mean steps 444.61 Mean score 28.73 Win 5.20% 3798.19s/1m steps\n",
      "Sp 460k/2m Mean losses 0.27 Mean winning sun 540.57 Mean steps 452.77 Mean score 29.11 Win 5.30% 3774.01s/1m steps\n",
      "Sp 470k/2m Mean losses 0.27 Mean winning sun 557.73 Mean steps 459.06 Mean score 29.40 Win 5.40% 3750.49s/1m steps\n",
      "Sp 480k/2m Mean losses 0.27 Mean winning sun 560.27 Mean steps 466.34 Mean score 29.73 Win 5.50% 3727.66s/1m steps\n",
      "Sp 490k/2m Mean losses 0.27 Mean winning sun 560.27 Mean steps 475.22 Mean score 30.12 Win 5.50% 3705.18s/1m steps\n",
      "Sp 500k/2m Mean losses 0.27 Mean winning sun 557.76 Mean steps 479.62 Mean score 30.17 Win 5.70% 3684.28s/1m steps\n",
      "Sp 510k/2m Mean losses 0.27 Mean winning sun 564.41 Mean steps 486.88 Mean score 30.65 Win 5.80% 3663.86s/1m steps\n",
      "Sp 520k/2m Mean losses 0.27 Mean winning sun 563.31 Mean steps 492.78 Mean score 31.08 Win 6.00% 3644.09s/1m steps\n",
      "Sp 530k/2m Mean losses 0.27 Mean winning sun 563.31 Mean steps 498.59 Mean score 31.23 Win 6.00% 3625.43s/1m steps\n",
      "Sp 540k/2m Mean losses 0.27 Mean winning sun 564.23 Mean steps 505.01 Mean score 31.55 Win 6.30% 3606.91s/1m steps\n",
      "Sp 550k/2m Mean losses 0.27 Mean winning sun 555.43 Mean steps 509.58 Mean score 31.89 Win 6.70% 3589.59s/1m steps\n",
      "Sp 560k/2m Mean losses 0.27 Mean winning sun 564.24 Mean steps 510.48 Mean score 32.05 Win 7.00% 3572.29s/1m steps\n",
      "Sp 570k/2m Mean losses 0.26 Mean winning sun 561.30 Mean steps 515.04 Mean score 32.31 Win 7.10% 3555.49s/1m steps\n",
      "Sp 580k/2m Mean losses 0.26 Mean winning sun 566.00 Mean steps 520.39 Mean score 32.59 Win 7.30% 3539.22s/1m steps\n",
      "Sp 590k/2m Mean losses 0.26 Mean winning sun 560.24 Mean steps 526.81 Mean score 33.12 Win 8.10% 3524.10s/1m steps\n",
      "Sp 600k/2m Mean losses 0.27 Mean winning sun 561.08 Mean steps 533.61 Mean score 33.56 Win 8.60% 3510.12s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 610k/2m Mean losses 0.27 Mean winning sun 558.52 Mean steps 541.26 Mean score 33.96 Win 8.90% 3888.99s/1m steps\n",
      "Sp 620k/2m Mean losses 0.26 Mean winning sun 570.36 Mean steps 549.74 Mean score 34.60 Win 9.50% 3869.53s/1m steps\n",
      "Sp 630k/2m Mean losses 0.26 Mean winning sun 578.28 Mean steps 557.29 Mean score 34.93 Win 9.70% 3850.16s/1m steps\n",
      "Sp 640k/2m Mean losses 0.26 Mean winning sun 584.05 Mean steps 562.62 Mean score 35.38 Win 10.30% 3831.83s/1m steps\n",
      "Sp 650k/2m Mean losses 0.26 Mean winning sun 580.61 Mean steps 567.00 Mean score 35.52 Win 10.40% 3813.18s/1m steps\n",
      "Sp 660k/2m Mean losses 0.26 Mean winning sun 577.78 Mean steps 571.79 Mean score 35.67 Win 10.50% 3795.36s/1m steps\n",
      "Sp 670k/2m Mean losses 0.26 Mean winning sun 573.88 Mean steps 569.38 Mean score 35.86 Win 10.80% 3778.68s/1m steps\n",
      "Sp 680k/2m Mean losses 0.26 Mean winning sun 568.48 Mean steps 563.14 Mean score 35.79 Win 11.00% 3762.21s/1m steps\n",
      "Sp 690k/2m Mean losses 0.26 Mean winning sun 569.66 Mean steps 566.52 Mean score 36.16 Win 11.20% 3746.01s/1m steps\n",
      "Sp 700k/2m Mean losses 0.26 Mean winning sun 570.62 Mean steps 567.85 Mean score 36.37 Win 11.40% 3730.76s/1m steps\n",
      "Sp 710k/2m Mean losses 0.26 Mean winning sun 574.60 Mean steps 567.42 Mean score 36.57 Win 11.70% 3715.35s/1m steps\n",
      "Sp 720k/2m Mean losses 0.26 Mean winning sun 578.54 Mean steps 571.18 Mean score 36.78 Win 12.00% 3700.24s/1m steps\n",
      "Sp 730k/2m Mean losses 0.26 Mean winning sun 575.00 Mean steps 570.85 Mean score 37.28 Win 12.80% 3686.05s/1m steps\n",
      "Sp 740k/2m Mean losses 0.26 Mean winning sun 570.00 Mean steps 568.08 Mean score 37.12 Win 13.10% 3672.09s/1m steps\n",
      "Sp 750k/2m Mean losses 0.26 Mean winning sun 582.17 Mean steps 570.72 Mean score 37.34 Win 13.10% 3659.13s/1m steps\n",
      "Sp 760k/2m Mean losses 0.26 Mean winning sun 584.18 Mean steps 575.39 Mean score 37.70 Win 13.50% 3645.83s/1m steps\n",
      "Sp 770k/2m Mean losses 0.26 Mean winning sun 578.12 Mean steps 572.74 Mean score 38.00 Win 13.90% 3632.91s/1m steps\n",
      "Sp 780k/2m Mean losses 0.26 Mean winning sun 581.09 Mean steps 565.94 Mean score 38.07 Win 14.10% 3620.59s/1m steps\n",
      "Sp 790k/2m Mean losses 0.26 Mean winning sun 586.82 Mean steps 558.47 Mean score 38.47 Win 15.00% 3608.41s/1m steps\n",
      "Sp 800k/2m Mean losses 0.26 Mean winning sun 587.65 Mean steps 550.23 Mean score 38.65 Win 15.70% 3603.90s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 810k/2m Mean losses 0.26 Mean winning sun 593.02 Mean steps 549.45 Mean score 39.34 Win 16.00% 3843.61s/1m steps\n",
      "Sp 820k/2m Mean losses 0.26 Mean winning sun 595.41 Mean steps 545.95 Mean score 39.28 Win 16.10% 3829.76s/1m steps\n",
      "Sp 830k/2m Mean losses 0.26 Mean winning sun 594.37 Mean steps 541.30 Mean score 39.53 Win 16.60% 3815.68s/1m steps\n",
      "Sp 840k/2m Mean losses 0.26 Mean winning sun 595.18 Mean steps 543.18 Mean score 39.52 Win 16.60% 3802.06s/1m steps\n",
      "Sp 850k/2m Mean losses 0.26 Mean winning sun 594.77 Mean steps 542.96 Mean score 39.82 Win 17.00% 3789.99s/1m steps\n",
      "Sp 860k/2m Mean losses 0.26 Mean winning sun 600.00 Mean steps 537.59 Mean score 40.10 Win 17.80% 3777.43s/1m steps\n",
      "Sp 870k/2m Mean losses 0.26 Mean winning sun 597.00 Mean steps 544.21 Mean score 40.30 Win 18.10% 3764.03s/1m steps\n",
      "Sp 880k/2m Mean losses 0.26 Mean winning sun 591.63 Mean steps 544.14 Mean score 40.53 Win 18.70% 3751.43s/1m steps\n",
      "Sp 890k/2m Mean losses 0.26 Mean winning sun 601.24 Mean steps 545.16 Mean score 41.10 Win 19.40% 3738.82s/1m steps\n",
      "Sp 900k/2m Mean losses 0.26 Mean winning sun 604.33 Mean steps 550.27 Mean score 41.43 Win 19.60% 3726.71s/1m steps\n",
      "Sp 910k/2m Mean losses 0.26 Mean winning sun 611.42 Mean steps 543.82 Mean score 41.78 Win 20.20% 3714.98s/1m steps\n",
      "Sp 920k/2m Mean losses 0.26 Mean winning sun 619.67 Mean steps 547.32 Mean score 42.45 Win 20.70% 3704.08s/1m steps\n",
      "Sp 930k/2m Mean losses 0.26 Mean winning sun 622.65 Mean steps 547.65 Mean score 42.64 Win 20.90% 3692.82s/1m steps\n",
      "Sp 940k/2m Mean losses 0.26 Mean winning sun 621.54 Mean steps 548.12 Mean score 42.02 Win 20.20% 3690.84s/1m steps\n",
      "Sp 950k/2m Mean losses 0.26 Mean winning sun 624.12 Mean steps 546.87 Mean score 42.60 Win 21.00% 3682.54s/1m steps\n",
      "Sp 960k/2m Mean losses 0.26 Mean winning sun 628.48 Mean steps 546.06 Mean score 43.37 Win 22.10% 3677.05s/1m steps\n",
      "Sp 970k/2m Mean losses 0.26 Mean winning sun 635.55 Mean steps 541.81 Mean score 43.83 Win 22.80% 3667.05s/1m steps\n",
      "Sp 980k/2m Mean losses 0.27 Mean winning sun 636.38 Mean steps 527.96 Mean score 44.35 Win 23.80% 3657.42s/1m steps\n",
      "Sp 990k/2m Mean losses 0.27 Mean winning sun 640.13 Mean steps 520.94 Mean score 44.65 Win 24.40% 3647.25s/1m steps\n",
      "Sp 1000k/2m Mean losses 0.27 Mean winning sun 639.53 Mean steps 514.98 Mean score 45.03 Win 25.20% 3637.64s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 1.01m/2m Mean losses 0.27 Mean winning sun 641.85 Mean steps 507.16 Mean score 45.58 Win 26.10% 3803.70s/1m steps\n",
      "Sp 1.02m/2m Mean losses 0.27 Mean winning sun 644.89 Mean steps 487.83 Mean score 45.71 Win 27.00% 3793.76s/1m steps\n",
      "Sp 1.03m/2m Mean losses 0.27 Mean winning sun 646.39 Mean steps 479.29 Mean score 45.91 Win 27.70% 3783.99s/1m steps\n",
      "Sp 1.04m/2m Mean losses 0.27 Mean winning sun 649.65 Mean steps 472.73 Mean score 46.10 Win 28.00% 3774.00s/1m steps\n",
      "Sp 1.05m/2m Mean losses 0.27 Mean winning sun 650.56 Mean steps 469.24 Mean score 46.27 Win 28.40% 3764.57s/1m steps\n",
      "Sp 1.06m/2m Mean losses 0.27 Mean winning sun 648.24 Mean steps 465.97 Mean score 46.45 Win 28.50% 3754.99s/1m steps\n",
      "Sp 1.07m/2m Mean losses 0.27 Mean winning sun 649.34 Mean steps 457.74 Mean score 46.39 Win 28.60% 3746.19s/1m steps\n",
      "Sp 1.08m/2m Mean losses 0.28 Mean winning sun 651.28 Mean steps 446.20 Mean score 46.58 Win 29.10% 3737.22s/1m steps\n",
      "Sp 1.09m/2m Mean losses 0.28 Mean winning sun 648.52 Mean steps 445.27 Mean score 47.13 Win 29.90% 3728.19s/1m steps\n",
      "Sp 1.1m/2m Mean losses 0.28 Mean winning sun 650.36 Mean steps 437.54 Mean score 47.86 Win 31.10% 3719.60s/1m steps\n",
      "Sp 1.11m/2m Mean losses 0.28 Mean winning sun 654.70 Mean steps 436.82 Mean score 48.36 Win 31.80% 3711.19s/1m steps\n",
      "Sp 1.12m/2m Mean losses 0.28 Mean winning sun 655.33 Mean steps 442.81 Mean score 48.80 Win 32.10% 3702.71s/1m steps\n",
      "Sp 1.13m/2m Mean losses 0.27 Mean winning sun 657.02 Mean steps 431.35 Mean score 48.73 Win 32.60% 3694.29s/1m steps\n",
      "Sp 1.14m/2m Mean losses 0.27 Mean winning sun 659.31 Mean steps 428.88 Mean score 48.85 Win 32.70% 3686.32s/1m steps\n",
      "Sp 1.15m/2m Mean losses 0.27 Mean winning sun 659.21 Mean steps 424.19 Mean score 48.56 Win 32.40% 3678.37s/1m steps\n",
      "Sp 1.16m/2m Mean losses 0.27 Mean winning sun 660.09 Mean steps 428.63 Mean score 48.77 Win 32.40% 3670.51s/1m steps\n",
      "Sp 1.17m/2m Mean losses 0.27 Mean winning sun 663.09 Mean steps 423.88 Mean score 48.88 Win 32.60% 3662.71s/1m steps\n",
      "Sp 1.18m/2m Mean losses 0.27 Mean winning sun 665.01 Mean steps 423.15 Mean score 49.33 Win 33.20% 3654.97s/1m steps\n",
      "Sp 1.19m/2m Mean losses 0.27 Mean winning sun 666.18 Mean steps 417.32 Mean score 49.70 Win 34.00% 3647.37s/1m steps\n",
      "Sp 1.2m/2m Mean losses 0.27 Mean winning sun 668.77 Mean steps 418.94 Mean score 49.90 Win 34.40% 3639.95s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 1.21m/2m Mean losses 0.27 Mean winning sun 669.57 Mean steps 420.49 Mean score 49.83 Win 34.20% 3839.76s/1m steps\n",
      "Sp 1.22m/2m Mean losses 0.27 Mean winning sun 671.33 Mean steps 421.79 Mean score 49.86 Win 34.10% 3830.93s/1m steps\n",
      "Sp 1.23m/2m Mean losses 0.27 Mean winning sun 674.33 Mean steps 424.79 Mean score 50.23 Win 34.60% 3821.84s/1m steps\n",
      "Sp 1.24m/2m Mean losses 0.27 Mean winning sun 674.86 Mean steps 429.68 Mean score 50.23 Win 34.60% 3813.05s/1m steps\n",
      "Sp 1.25m/2m Mean losses 0.27 Mean winning sun 677.06 Mean steps 431.86 Mean score 50.78 Win 35.20% 3804.55s/1m steps\n",
      "Sp 1.26m/2m Mean losses 0.27 Mean winning sun 678.12 Mean steps 435.20 Mean score 51.06 Win 35.60% 3796.31s/1m steps\n",
      "Sp 1.27m/2m Mean losses 0.27 Mean winning sun 676.12 Mean steps 434.73 Mean score 51.66 Win 36.50% 3788.18s/1m steps\n",
      "Sp 1.28m/2m Mean losses 0.27 Mean winning sun 678.19 Mean steps 431.03 Mean score 51.91 Win 36.80% 3780.22s/1m steps\n",
      "Sp 1.29m/2m Mean losses 0.27 Mean winning sun 680.03 Mean steps 431.22 Mean score 52.15 Win 37.20% 3772.48s/1m steps\n",
      "Sp 1.3m/2m Mean losses 0.27 Mean winning sun 678.83 Mean steps 429.40 Mean score 52.14 Win 37.20% 3764.68s/1m steps\n",
      "Sp 1.31m/2m Mean losses 0.27 Mean winning sun 676.61 Mean steps 431.40 Mean score 52.50 Win 37.60% 3756.97s/1m steps\n",
      "Sp 1.32m/2m Mean losses 0.27 Mean winning sun 674.96 Mean steps 430.12 Mean score 52.43 Win 37.80% 3749.50s/1m steps\n",
      "Sp 1.33m/2m Mean losses 0.27 Mean winning sun 672.06 Mean steps 423.81 Mean score 52.78 Win 38.50% 3741.96s/1m steps\n",
      "Sp 1.34m/2m Mean losses 0.27 Mean winning sun 670.46 Mean steps 422.92 Mean score 52.80 Win 38.60% 3734.91s/1m steps\n",
      "Sp 1.35m/2m Mean losses 0.27 Mean winning sun 670.25 Mean steps 418.87 Mean score 53.29 Win 39.20% 3727.91s/1m steps\n",
      "Sp 1.36m/2m Mean losses 0.27 Mean winning sun 672.74 Mean steps 412.29 Mean score 53.90 Win 40.10% 3720.75s/1m steps\n",
      "Sp 1.37m/2m Mean losses 0.27 Mean winning sun 673.62 Mean steps 408.48 Mean score 53.83 Win 40.10% 3714.37s/1m steps\n",
      "Sp 1.38m/2m Mean losses 0.27 Mean winning sun 674.96 Mean steps 404.83 Mean score 53.75 Win 40.00% 3707.51s/1m steps\n",
      "Sp 1.39m/2m Mean losses 0.27 Mean winning sun 675.11 Mean steps 410.07 Mean score 54.52 Win 40.60% 3701.05s/1m steps\n",
      "Sp 1.4m/2m Mean losses 0.27 Mean winning sun 675.57 Mean steps 414.80 Mean score 55.04 Win 41.10% 3694.27s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 1.41m/2m Mean losses 0.27 Mean winning sun 676.32 Mean steps 414.21 Mean score 55.75 Win 41.80% 3851.95s/1m steps\n",
      "Sp 1.42m/2m Mean losses 0.27 Mean winning sun 675.83 Mean steps 419.42 Mean score 55.86 Win 41.70% 3844.33s/1m steps\n",
      "Sp 1.43m/2m Mean losses 0.27 Mean winning sun 676.91 Mean steps 423.62 Mean score 56.09 Win 41.90% 3836.55s/1m steps\n",
      "Sp 1.44m/2m Mean losses 0.27 Mean winning sun 677.68 Mean steps 427.98 Mean score 56.55 Win 42.30% 3829.00s/1m steps\n",
      "Sp 1.45m/2m Mean losses 0.27 Mean winning sun 680.20 Mean steps 430.99 Mean score 57.23 Win 43.00% 3821.48s/1m steps\n",
      "Sp 1.46m/2m Mean losses 0.27 Mean winning sun 679.15 Mean steps 435.25 Mean score 57.60 Win 43.30% 3814.17s/1m steps\n",
      "Sp 1.47m/2m Mean losses 0.27 Mean winning sun 677.74 Mean steps 436.39 Mean score 58.13 Win 44.00% 3806.86s/1m steps\n",
      "Sp 1.48m/2m Mean losses 0.27 Mean winning sun 678.45 Mean steps 434.50 Mean score 58.59 Win 44.70% 3800.03s/1m steps\n",
      "Sp 1.49m/2m Mean losses 0.27 Mean winning sun 679.60 Mean steps 432.52 Mean score 58.97 Win 45.20% 3793.11s/1m steps\n",
      "Sp 1.5m/2m Mean losses 0.27 Mean winning sun 680.11 Mean steps 434.18 Mean score 59.27 Win 45.40% 3786.07s/1m steps\n",
      "Sp 1.51m/2m Mean losses 0.27 Mean winning sun 679.84 Mean steps 432.68 Mean score 59.48 Win 45.70% 3779.35s/1m steps\n",
      "Sp 1.52m/2m Mean losses 0.27 Mean winning sun 678.19 Mean steps 434.35 Mean score 59.33 Win 45.60% 3772.43s/1m steps\n",
      "Sp 1.53m/2m Mean losses 0.27 Mean winning sun 676.12 Mean steps 433.66 Mean score 58.88 Win 45.20% 3765.63s/1m steps\n",
      "Sp 1.54m/2m Mean losses 0.27 Mean winning sun 674.21 Mean steps 428.00 Mean score 58.84 Win 45.20% 3758.97s/1m steps\n",
      "Sp 1.55m/2m Mean losses 0.27 Mean winning sun 675.31 Mean steps 425.04 Mean score 59.32 Win 45.30% 3752.49s/1m steps\n",
      "Sp 1.56m/2m Mean losses 0.27 Mean winning sun 673.49 Mean steps 419.51 Mean score 59.49 Win 45.80% 3746.00s/1m steps\n",
      "Sp 1.57m/2m Mean losses 0.28 Mean winning sun 671.88 Mean steps 417.54 Mean score 59.77 Win 46.30% 3739.69s/1m steps\n",
      "Sp 1.58m/2m Mean losses 0.27 Mean winning sun 669.42 Mean steps 413.76 Mean score 60.06 Win 47.00% 3733.68s/1m steps\n",
      "Sp 1.59m/2m Mean losses 0.27 Mean winning sun 668.95 Mean steps 410.25 Mean score 60.28 Win 47.30% 3727.73s/1m steps\n",
      "Sp 1.6m/2m Mean losses 0.27 Mean winning sun 670.08 Mean steps 408.40 Mean score 59.95 Win 46.80% 3721.51s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 1.61m/2m Mean losses 0.28 Mean winning sun 668.74 Mean steps 405.14 Mean score 60.39 Win 47.50% 3799.94s/1m steps\n",
      "Sp 1.62m/2m Mean losses 0.28 Mean winning sun 671.04 Mean steps 395.96 Mean score 61.10 Win 48.50% 3793.66s/1m steps\n",
      "Sp 1.63m/2m Mean losses 0.27 Mean winning sun 674.17 Mean steps 391.81 Mean score 61.24 Win 48.90% 3787.72s/1m steps\n",
      "Sp 1.64m/2m Mean losses 0.27 Mean winning sun 673.98 Mean steps 384.26 Mean score 61.36 Win 49.30% 3781.56s/1m steps\n",
      "Sp 1.65m/2m Mean losses 0.27 Mean winning sun 678.45 Mean steps 379.35 Mean score 61.34 Win 49.40% 3775.63s/1m steps\n",
      "Sp 1.66m/2m Mean losses 0.27 Mean winning sun 681.17 Mean steps 377.94 Mean score 61.65 Win 49.80% 3769.77s/1m steps\n",
      "Sp 1.67m/2m Mean losses 0.27 Mean winning sun 680.98 Mean steps 373.35 Mean score 61.94 Win 50.50% 3764.03s/1m steps\n",
      "Sp 1.68m/2m Mean losses 0.27 Mean winning sun 680.65 Mean steps 374.43 Mean score 62.35 Win 50.90% 3758.40s/1m steps\n",
      "Sp 1.69m/2m Mean losses 0.28 Mean winning sun 685.42 Mean steps 362.53 Mean score 62.63 Win 51.30% 3752.66s/1m steps\n",
      "Sp 1.7m/2m Mean losses 0.28 Mean winning sun 687.08 Mean steps 359.69 Mean score 62.78 Win 51.80% 3746.69s/1m steps\n",
      "Sp 1.71m/2m Mean losses 0.28 Mean winning sun 689.42 Mean steps 356.69 Mean score 62.78 Win 52.20% 3740.75s/1m steps\n",
      "Sp 1.72m/2m Mean losses 0.28 Mean winning sun 691.92 Mean steps 354.07 Mean score 62.96 Win 52.60% 3735.32s/1m steps\n",
      "Sp 1.73m/2m Mean losses 0.28 Mean winning sun 690.88 Mean steps 355.69 Mean score 63.06 Win 53.00% 3729.52s/1m steps\n",
      "Sp 1.74m/2m Mean losses 0.28 Mean winning sun 693.08 Mean steps 358.08 Mean score 63.15 Win 53.30% 3723.74s/1m steps\n",
      "Sp 1.75m/2m Mean losses 0.28 Mean winning sun 691.85 Mean steps 345.25 Mean score 62.43 Win 52.90% 3718.06s/1m steps\n",
      "Sp 1.76m/2m Mean losses 0.28 Mean winning sun 693.23 Mean steps 336.41 Mean score 62.76 Win 53.60% 3712.44s/1m steps\n",
      "Sp 1.77m/2m Mean losses 0.28 Mean winning sun 691.80 Mean steps 324.76 Mean score 62.45 Win 53.90% 3706.95s/1m steps\n",
      "Sp 1.78m/2m Mean losses 0.28 Mean winning sun 692.27 Mean steps 319.87 Mean score 62.31 Win 53.80% 3701.60s/1m steps\n",
      "Sp 1.79m/2m Mean losses 0.28 Mean winning sun 693.05 Mean steps 316.72 Mean score 62.91 Win 54.60% 3696.28s/1m steps\n",
      "Sp 1.8m/2m Mean losses 0.28 Mean winning sun 688.92 Mean steps 312.34 Mean score 62.15 Win 54.40% 3691.20s/1m steps\n",
      "Testing 500/500...\n",
      "Sp 1.81m/2m Mean losses 0.28 Mean winning sun 692.73 Mean steps 306.20 Mean score 62.27 Win 54.90% 3756.78s/1m steps\n",
      "Sp 1.82m/2m Mean losses 0.29 Mean winning sun 694.80 Mean steps 303.01 Mean score 63.00 Win 55.80% 3751.31s/1m steps\n",
      "Sp 1.83m/2m Mean losses 0.29 Mean winning sun 693.45 Mean steps 305.44 Mean score 63.98 Win 56.70% 3745.60s/1m steps\n",
      "Sp 1.84m/2m Mean losses 0.29 Mean winning sun 693.40 Mean steps 304.25 Mean score 64.66 Win 57.60% 3742.07s/1m steps\n",
      "Sp 1.85m/2m Mean losses 0.29 Mean winning sun 693.85 Mean steps 300.95 Mean score 64.53 Win 58.30% 3737.28s/1m steps\n",
      "Sp 1.86m/2m Mean losses 0.29 Mean winning sun 695.27 Mean steps 300.15 Mean score 65.44 Win 59.30% 3732.90s/1m steps\n",
      "Sp 1.87m/2m Mean losses 0.29 Mean winning sun 695.62 Mean steps 294.35 Mean score 65.96 Win 60.10% 3728.08s/1m steps\n",
      "Sp 1.88m/2m Mean losses 0.29 Mean winning sun 695.55 Mean steps 288.80 Mean score 65.69 Win 60.10% 3723.53s/1m steps\n",
      "Sp 1.89m/2m Mean losses 0.29 Mean winning sun 696.52 Mean steps 279.68 Mean score 65.33 Win 60.00% 3718.67s/1m steps\n",
      "Sp 1.9m/2m Mean losses 0.29 Mean winning sun 701.90 Mean steps 277.38 Mean score 64.83 Win 59.80% 3713.47s/1m steps\n",
      "Sp 1.91m/2m Mean losses 0.29 Mean winning sun 703.45 Mean steps 273.53 Mean score 64.41 Win 59.70% 3710.44s/1m steps\n",
      "Sp 1.92m/2m Mean losses 0.29 Mean winning sun 705.10 Mean steps 274.65 Mean score 64.77 Win 60.00% 3705.64s/1m steps\n",
      "Sp 1.93m/2m Mean losses 0.29 Mean winning sun 706.88 Mean steps 270.68 Mean score 65.06 Win 60.40% 3704.41s/1m steps\n",
      "Sp 1.94m/2m Mean losses 0.29 Mean winning sun 707.42 Mean steps 259.87 Mean score 64.27 Win 59.40% 3699.55s/1m steps\n",
      "Sp 1.95m/2m Mean losses 0.29 Mean winning sun 708.85 Mean steps 260.35 Mean score 63.97 Win 59.20% 3694.65s/1m steps\n",
      "Sp 1.96m/2m Mean losses 0.29 Mean winning sun 710.30 Mean steps 256.81 Mean score 63.54 Win 58.80% 3689.68s/1m steps\n",
      "Sp 1.97m/2m Mean losses 0.29 Mean winning sun 710.35 Mean steps 255.99 Mean score 63.59 Win 58.70% 3685.85s/1m steps\n",
      "Sp 1.98m/2m Mean losses 0.29 Mean winning sun 714.73 Mean steps 252.93 Mean score 63.24 Win 58.20% 3681.16s/1m steps\n",
      "Sp 1.99m/2m Mean losses 0.29 Mean winning sun 715.45 Mean steps 249.24 Mean score 63.00 Win 57.90% 3676.57s/1m steps\n",
      "Sp 2m/2m Mean losses 0.29 Mean winning sun 714.38 Mean steps 249.21 Mean score 63.19 Win 58.40% 3672.05s/1m steps\n",
      "Testing 178/500..."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    def seed_torch(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.backends.cudnn.enabled:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    seed_torch(seed)\n",
    "\n",
    "\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "env = IZenv()\n",
    "\n",
    "num_steps = 2_000_000\n",
    "agent = DQNAgent(\n",
    "    env,\n",
    "    device=\"cuda\",\n",
    "    model_name=\"r2\",\n",
    "    memory_size=1_000_000,\n",
    "    batch_size=256,\n",
    "    gamma=0.99,\n",
    "    alpha=0.2,\n",
    "    beta=0.6,\n",
    "    prior_eps=1e-6,\n",
    "    v_min=-11,\n",
    "    v_max=80,\n",
    "    atom_size=None,\n",
    "    n_step=3,\n",
    "    lr=1e-3,\n",
    ")\n",
    "# agent.load(\"model/r1_2023.11.19_00.55.17/42.5m.pth\")\n",
    "# manually_test_agent(agent, fix_rand=False)\n",
    "agent.train(\n",
    "    update_target_every=2000,\n",
    "    update_main_every=16,\n",
    "    num_steps=num_steps,\n",
    "    print_stats_every=10_000,\n",
    "    save_every=100_000,\n",
    "    eval_every=200_000,\n",
    ")\n",
    "# manually_test_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.save_stats_to_csv(\"RainbowDQN(n=3).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.plot_stats_in_one_graph(\"Performance Over Steps for RainbowDQN (n=3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_stats_winning_rate(\"Winning Rate Over Steps for RainbowDQN (n=3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_stats_remaining_suns(\"Average Remaining Suns Over Steps for RainbowDQN (n=3)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tXwXYckvViOv",
    "ZwuPV_EGVr1U"
   ],
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1QN5f6jlyUnK01ze7FfnMPtTV3idSqfIv",
     "timestamp": 1701487140203
    },
    {
     "file_id": "1osKdj0NZ7pEPboKO8OGBY4CN9RbyIAel",
     "timestamp": 1700943976756
    },
    {
     "file_id": "1QQWUWXai-SIRGqAOD3soYMLpY7LtiE2C",
     "timestamp": 1700937785634
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
